[2022-10-10 14:46:46,437] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2022-10-10 14:46:46,805] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2022-10-10 14:46:47,067] INFO [MergedLog partition=__cluster_metadata-0, dir=/tmp/server2/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2022-10-10 14:46:47,068] INFO [MergedLog partition=__cluster_metadata-0, dir=/tmp/server2/logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.MergedLog$)
[2022-10-10 14:46:47,068] INFO [MergedLog partition=__cluster_metadata-0, dir=/tmp/server2/logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.MergedLog$)
[2022-10-10 14:46:47,110] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2022-10-10 14:46:47,129] INFO [RaftManager nodeId=2] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1249) (org.apache.kafka.raft.QuorumState)
[2022-10-10 14:46:47,141] INFO Instantiating ClusterBalanceManager with an instance of io.confluent.databalancer.SbcDataBalanceManager (ClusterBalanceManager)
[2022-10-10 14:46:47,160] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://localhost:9093
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 2
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = ucxex-roTpKFD1r9JRvnNw
	client.quota.callback.class = null
	compression.type = producer
	confluent.ansible.managed = false
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.persistence.store.startup.max.retry.ms = 300000
	confluent.balancer.api.state.persistence.store.startup.retry.wait.ms = 100
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.enable = false
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.replication.factor = 3
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.external.network.mititgation.enabled = false
	confluent.broker.health.manager.external.network.sample.duration.ms = 1000
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.healthy = 120
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.num.samples.before.network.unhealthy = 120
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.cdc.api.keys.load.timeout.ms = 600000000000
	confluent.cdc.api.keys.topic = 
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.enable = false
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 3
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.connection.invalid.request.delay.enable = false
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.partition.pruning.enable = true
	confluent.group.metadata.load.threads = 32
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.log.placement.constraints = 
	confluent.max.connection.creation.rate.per.ip = 2147483647
	confluent.max.connection.throttle.ms = null
	confluent.metadata.active.encryptor = null
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.max.partitions.per.request = 2147483647
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.offsets.topic.placement.constraints = 
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.prefer.tier.fetch.ms = -1
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.require.compatible.keystore.updates = true
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config = 
	confluent.segment.speculative.prefetch.enable = false
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.storage.probe.period.ms = -1
	confluent.telemetry.enabled = false
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.topic.replica.assignor.builder.class = 
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints = 
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@localhost:19092, 2@localhost:19093, 3@localhost:19094]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 3
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.2-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://:9093,CONTROLLER://:19093
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/server2/logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 2
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 2
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker, controller]
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2022-10-10 14:46:47,176] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://localhost:9093
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 2
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = ucxex-roTpKFD1r9JRvnNw
	client.quota.callback.class = null
	compression.type = producer
	confluent.ansible.managed = false
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.persistence.store.startup.max.retry.ms = 300000
	confluent.balancer.api.state.persistence.store.startup.retry.wait.ms = 100
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.enable = false
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.replication.factor = 3
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.external.network.mititgation.enabled = false
	confluent.broker.health.manager.external.network.sample.duration.ms = 1000
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.healthy = 120
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.num.samples.before.network.unhealthy = 120
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.cdc.api.keys.load.timeout.ms = 600000000000
	confluent.cdc.api.keys.topic = 
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.enable = false
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 3
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.connection.invalid.request.delay.enable = false
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.partition.pruning.enable = true
	confluent.group.metadata.load.threads = 32
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.log.placement.constraints = 
	confluent.max.connection.creation.rate.per.ip = 2147483647
	confluent.max.connection.throttle.ms = null
	confluent.metadata.active.encryptor = null
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.max.partitions.per.request = 2147483647
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.offsets.topic.placement.constraints = 
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.prefer.tier.fetch.ms = -1
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.require.compatible.keystore.updates = true
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config = 
	confluent.segment.speculative.prefetch.enable = false
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.storage.probe.period.ms = -1
	confluent.telemetry.enabled = false
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.topic.replica.assignor.builder.class = 
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints = 
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@localhost:19092, 2@localhost:19093, 3@localhost:19094]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 3
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.2-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://:9093,CONTROLLER://:19093
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/server2/logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 2
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 2
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker, controller]
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2022-10-10 14:46:47,181] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2022-10-10 14:46:47,183] INFO [kafka-raft-outbound-request-thread]: Starting (kafka.raft.RaftSendThread)
[2022-10-10 14:46:47,183] INFO [kafka-raft-io-thread]: Starting (kafka.raft.KafkaRaftManager$RaftIoThread)
[2022-10-10 14:46:47,183] INFO Starting controller (kafka.server.ControllerServer)
[2022-10-10 14:46:47,299] INFO Updated connection-tokens max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2022-10-10 14:46:47,305] INFO Awaiting socket connections on 0.0.0.0:19093. (kafka.network.DataPlaneAcceptor)
[2022-10-10 14:46:47,318] INFO [SocketServer listenerType=CONTROLLER, nodeId=2] Created data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer)
[2022-10-10 14:46:47,353] INFO [RaftManager nodeId=2] Registered the listener org.apache.kafka.controller.QuorumController$QuorumMetaLogListener@212642530 (org.apache.kafka.raft.KafkaRaftClient)
[2022-10-10 14:46:47,359] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2022-10-10 14:46:47,361] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2022-10-10 14:46:47,362] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2022-10-10 14:46:47,362] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2022-10-10 14:46:47,364] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2022-10-10 14:46:47,367] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2022-10-10 14:46:47,367] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2022-10-10 14:46:47,375] INFO [ExpirationReaper-2-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2022-10-10 14:46:47,377] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.samples.per.min = 0
 (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
[2022-10-10 14:46:47,382] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
 (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
[2022-10-10 14:46:47,383] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.samples.per.min = 0
 (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
[2022-10-10 14:46:47,383] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
 (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
[2022-10-10 14:46:47,383] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.samples.per.min = 0
 (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
[2022-10-10 14:46:47,383] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
 (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
[2022-10-10 14:46:47,384] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.samples.per.min = 0
 (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
[2022-10-10 14:46:47,384] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
 (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
[2022-10-10 14:46:47,384] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.samples.per.min = 0
 (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
[2022-10-10 14:46:47,384] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
 (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
[2022-10-10 14:46:47,384] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.samples.per.min = 0
 (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
[2022-10-10 14:46:47,384] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
 (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
[2022-10-10 14:46:47,384] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.samples.per.min = 0
 (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
[2022-10-10 14:46:47,385] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
 (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
[2022-10-10 14:46:47,385] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.samples.per.min = 0
 (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
[2022-10-10 14:46:47,385] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
 (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
[2022-10-10 14:46:47,385] INFO [SocketServer listenerType=CONTROLLER, nodeId=2] Starting socket server acceptors and processors (kafka.network.SocketServer)
[2022-10-10 14:46:47,388] INFO [SocketServer listenerType=CONTROLLER, nodeId=2] Started data-plane acceptor and processor(s) for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer)
[2022-10-10 14:46:47,388] INFO [SocketServer listenerType=CONTROLLER, nodeId=2] Started socket server acceptors and processors (kafka.network.SocketServer)
[2022-10-10 14:46:47,388] INFO [BrokerServer id=2] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2022-10-10 14:46:47,389] INFO [BrokerServer id=2] Starting broker (kafka.server.BrokerServer)
[2022-10-10 14:46:47,389] INFO [BrokerServer id=2] FIPS mode enabled: false (kafka.server.BrokerServer)
[2022-10-10 14:46:47,402] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2022-10-10 14:46:47,402] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2022-10-10 14:46:47,406] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2022-10-10 14:46:47,406] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2022-10-10 14:46:47,407] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2022-10-10 14:46:47,407] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2022-10-10 14:46:47,407] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2022-10-10 14:46:47,430] INFO AuditLogConfig values: 
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.cloudevent.codec = structured
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
	confluent.security.event.logger.exporter.kafka.topic.create = true
	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
	confluent.security.event.router.cache.entries = 10000
	confluent.security.event.router.config = 
 (io.confluent.security.audit.AuditLogConfig)
[2022-10-10 14:46:47,430] INFO AuditLogConfig values: 
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.cloudevent.codec = structured
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
	confluent.security.event.logger.exporter.kafka.topic.create = true
	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
	confluent.security.event.router.cache.entries = 10000
	confluent.security.event.router.config = 
 (io.confluent.security.audit.AuditLogConfig)
[2022-10-10 14:46:47,442] INFO CrnAuthorityConfig values: 
	confluent.authorizer.authority.cache.entries = 10000
	confluent.authorizer.authority.name = 
	confluent.metadata.server.api.flavor = CP
 (io.confluent.crn.CrnAuthorityConfig)
[2022-10-10 14:46:47,443] INFO CrnAuthorityConfig values: 
	confluent.authorizer.authority.cache.entries = 10000
	confluent.authorizer.authority.name = 
	confluent.metadata.server.api.flavor = CP
 (io.confluent.crn.CrnAuthorityConfig)
[2022-10-10 14:46:47,443] INFO MultiTenantAuditLogConfig values: 
	confluent.security.event.logger.client.ip.enable = false
	confluent.security.event.logger.multitenant.enable = false
 (io.confluent.kafka.multitenant.authorizer.MultiTenantAuditLogConfig)
[2022-10-10 14:46:47,449] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Starting (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:46:47,483] INFO Updated connection-tokens max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2022-10-10 14:46:47,483] INFO Awaiting socket connections on 0.0.0.0:9093. (kafka.network.DataPlaneAcceptor)
[2022-10-10 14:46:47,489] INFO [SocketServer listenerType=BROKER, nodeId=2] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2022-10-10 14:46:47,497] INFO [BrokerToControllerChannelManager broker=2 name=alterIsr]: Starting (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:46:47,510] INFO [ExpirationReaper-2-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2022-10-10 14:46:47,512] INFO [ExpirationReaper-2-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2022-10-10 14:46:47,512] INFO [ExpirationReaper-2-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2022-10-10 14:46:47,512] INFO [ExpirationReaper-2-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2022-10-10 14:46:47,513] INFO [ExpirationReaper-2-ListOffsets]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2022-10-10 14:46:47,530] INFO [ExpirationReaper-2-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2022-10-10 14:46:47,531] INFO [ExpirationReaper-2-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2022-10-10 14:46:47,569] INFO EventEmitterConfig values: 
 (io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig)
[2022-10-10 14:46:47,606] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig)
[2022-10-10 14:46:47,699] WARN Ignoring redefinition of existing telemetry label kafka.version (io.confluent.shaded.io.confluent.telemetry.ResourceBuilderFacade)
[2022-10-10 14:46:47,711] INFO ConfluentTelemetryConfig values: 
	confluent.telemetry.api.key = null
	confluent.telemetry.api.secret = null
	confluent.telemetry.debug.enabled = false
	confluent.telemetry.enabled = false
	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
	confluent.telemetry.events.enable = true
	confluent.telemetry.metrics.collector.include = .*io.confluent.telemetry/.*.*|.*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|committed|used).*|.*io.confluent.kafka.server/.*(confluent_audit/audit_log_fallback_rate_per_minute|confluent_audit/audit_log_rate_per_minute|confluent_authorizer/authorization_request_rate_per_minute|confluent_authorizer/authorization_allowed_rate_per_minute|confluent_authorizer/authorization_denied_rate_per_minute|confluent_auth_store/rbac_role_bindings_count|confluent_auth_store/rbac_access_rules_count|confluent_auth_store/acl_access_rules_count).*|.*io.confluent.kafka.server/.*(acl_authorizer/zookeeper_disconnects/total/delta|acl_authorizer/zookeeper_expires/total/delta|broker_failure/zookeeper_disconnects/total/delta|broker_failure/zookeeper_expires/total/delta|broker_topic/bytes_in/total/delta|broker_topic/bytes_out/total/delta|broker_topic/failed_produce_requests/total/delta|broker_topic/failed_fetch_requests/total/delta|broker_topic/produce_message_conversions/total/delta|broker_topic/fetch_message_conversions/total/delta|controller/active_controller_count|controller/leader_election_rate_and_time_ms|controller/offline_partitions_count|controller/partition_availability|controller/global_under_min_isr_partition_count|controller/unclean_leader_elections/total|controller_channel/connection_close_rate|controller_channel/connection_close_total|controller_channel/connection_count|controller_channel/connection_creation_rate|controller_channel/connection_creation_total|controller_channel/request_size_avg|controller_channel/request_size_max|controller_channel_manager/queue_size|controller_channel_manager/total_queue_size|controller_event_manager/event_queue_size|delayed_operation_purgatory/purgatory_size|executor/zookeeper_disconnects/total/delta|executor/zookeeper_expires/total/delta|fetch/queue_size|group_coordinator/partition_load_time_max|log_cleaner_manager/achieved_cleaning_ratio/time/delta|log_cleaner_manager/achieved_cleaning_ratio/total/delta|log_cleaner_manager/compacted_partition_bytes|log_cleaner_manager/max_dirty_percent|log_cleaner_manager/time_since_last_run_ms|log_cleaner_manager/uncleanable_bytes|log_cleaner_manager/uncleanable_partitions_count|replica_alter_log_dirs_manager/max_lag|replica_fetcher/request_size_avg|replica_fetcher/request_size_max|replica_fetcher_manager/max_lag|replica_manager/isr_shrinks|replica_manager/leader_count|replica_manager/partition_count|replica_manager/under_min_isr_partition_count|replica_manager/under_replicated_partitions|request/errors/total/delta|request/local_time_ms/time/delta|request/local_time_ms/total/delta|request/queue_size|request/remote_time_ms/time/delta|request/remote_time_ms/total/delta|request/request_queue_time_ms/time/delta|request/request_queue_time_ms/total/delta|request/requests|request/response_queue_time_ms/time/delta|request/response_queue_time_ms/total/delta|request/response_send_time_ms/time/delta|request/response_send_time_ms/total/delta|request/total_time_ms/time/delta|request/total_time_ms/total/delta|request_channel/request_queue_size|request_channel/response_queue_size|request_handler_pool/request_handler_avg_idle_percent|session_expire_listener/zookeeper_disconnects/total/delta|session_expire_listener/zookeeper_expires/total/delta|socket_server/connections|socket_server/successful_authentication_total/delta|socket_server/failed_authentication_total/delta|socket_server/network_processor_avg_idle_percent|socket_server/request_size_avg|socket_server/request_size_max).*
	confluent.telemetry.metrics.collector.interval.ms = 60000
	confluent.telemetry.metrics.collector.slo.enabled = false
	confluent.telemetry.proxy.password = null
	confluent.telemetry.proxy.url = null
	confluent.telemetry.proxy.username = null
 (io.confluent.telemetry.ConfluentTelemetryConfig)
[2022-10-10 14:46:47,712] INFO VolumeMetricsCollectorConfig values: 
	confluent.telemetry.metrics.collector.volume.update.ms = 15000
 (io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig)
[2022-10-10 14:46:47,712] INFO HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	metrics.include = .*io.confluent.telemetry/.*.*|.*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|committed|used).*|.*io.confluent.kafka.server/.*(confluent_audit/audit_log_fallback_rate_per_minute|confluent_audit/audit_log_rate_per_minute|confluent_authorizer/authorization_request_rate_per_minute|confluent_authorizer/authorization_allowed_rate_per_minute|confluent_authorizer/authorization_denied_rate_per_minute|confluent_auth_store/rbac_role_bindings_count|confluent_auth_store/rbac_access_rules_count|confluent_auth_store/acl_access_rules_count).*|.*io.confluent.kafka.server/.*(acl_authorizer/zookeeper_disconnects/total/delta|acl_authorizer/zookeeper_expires/total/delta|broker_failure/zookeeper_disconnects/total/delta|broker_failure/zookeeper_expires/total/delta|broker_topic/bytes_in/total/delta|broker_topic/bytes_out/total/delta|broker_topic/failed_produce_requests/total/delta|broker_topic/failed_fetch_requests/total/delta|broker_topic/produce_message_conversions/total/delta|broker_topic/fetch_message_conversions/total/delta|controller/active_controller_count|controller/leader_election_rate_and_time_ms|controller/offline_partitions_count|controller/partition_availability|controller/global_under_min_isr_partition_count|controller/unclean_leader_elections/total|controller_channel/connection_close_rate|controller_channel/connection_close_total|controller_channel/connection_count|controller_channel/connection_creation_rate|controller_channel/connection_creation_total|controller_channel/request_size_avg|controller_channel/request_size_max|controller_channel_manager/queue_size|controller_channel_manager/total_queue_size|controller_event_manager/event_queue_size|delayed_operation_purgatory/purgatory_size|executor/zookeeper_disconnects/total/delta|executor/zookeeper_expires/total/delta|fetch/queue_size|group_coordinator/partition_load_time_max|log_cleaner_manager/achieved_cleaning_ratio/time/delta|log_cleaner_manager/achieved_cleaning_ratio/total/delta|log_cleaner_manager/compacted_partition_bytes|log_cleaner_manager/max_dirty_percent|log_cleaner_manager/time_since_last_run_ms|log_cleaner_manager/uncleanable_bytes|log_cleaner_manager/uncleanable_partitions_count|replica_alter_log_dirs_manager/max_lag|replica_fetcher/request_size_avg|replica_fetcher/request_size_max|replica_fetcher_manager/max_lag|replica_manager/isr_shrinks|replica_manager/leader_count|replica_manager/partition_count|replica_manager/under_min_isr_partition_count|replica_manager/under_replicated_partitions|request/errors/total/delta|request/local_time_ms/time/delta|request/local_time_ms/total/delta|request/queue_size|request/remote_time_ms/time/delta|request/remote_time_ms/total/delta|request/request_queue_time_ms/time/delta|request/request_queue_time_ms/total/delta|request/requests|request/response_queue_time_ms/time/delta|request/response_queue_time_ms/total/delta|request/response_send_time_ms/time/delta|request/response_send_time_ms/total/delta|request/total_time_ms/time/delta|request/total_time_ms/total/delta|request_channel/request_queue_size|request_channel/response_queue_size|request_handler_pool/request_handler_avg_idle_percent|session_expire_listener/zookeeper_disconnects/total/delta|session_expire_listener/zookeeper_expires/total/delta|socket_server/connections|socket_server/successful_authentication_total/delta|socket_server/failed_authentication_total/delta|socket_server/network_processor_avg_idle_percent|socket_server/request_size_avg|socket_server/request_size_max).*
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = http
 (io.confluent.telemetry.exporter.http.HttpExporterConfig)
[2022-10-10 14:46:47,714] INFO KafkaExporterConfig values: 
	enabled = true
	events.enabled = true
	metrics.enabled = true
	metrics.include = (io\.confluent\.kafka\.server/broker_topic/bytes_in/rate/1_min|io\.confluent\.kafka\.server/broker_topic/bytes_out/rate/1_min|io\.confluent\.kafka\.server/broker_topic/messages_in/rate/1_min|io\.confluent\.kafka\.server/broker_topic/replication_bytes_in/rate/1_min|io\.confluent\.kafka\.server/broker_topic/replication_bytes_out/rate/1_min|io\.confluent\.kafka\.server/broker_topic/total_fetch_requests/rate/1_min|io\.confluent\.kafka\.server/broker_topic/total_produce_requests/rate/1_min|io\.confluent\.kafka\.server/log/size|io\.confluent\.kafka\.server/log_flush/log_flush_rate_and_time_ms|io\.confluent\.kafka\.server/log_flush/log_flush_rate_and_time_ms/rate/1_min|io\.confluent\.kafka\.server/request/local_time_ms|io\.confluent\.kafka\.server/request/request_queue_time_ms|io\.confluent\.kafka\.server/request/requests/rate/1_min|io\.confluent\.kafka\.server/request/total_time_ms|io\.confluent\.kafka\.server/request_channel/request_queue_size|io\.confluent\.kafka\.server/request_channel/response_queue_size|io\.confluent\.kafka\.server/request_handler_pool/request_handler_avg_idle_percent/rate/1_min|io\.confluent\.system/jvm/os/process_cpu_load|io\.confluent\.system/volume/disk_total_bytes)
	producer.bootstrap.servers = localhost:9093
	topic.create = true
	topic.max.message.bytes = 10485760
	topic.name = _confluent-telemetry-metrics
	topic.partitions = 12
	topic.replicas = 3
	topic.retention.bytes = -1
	topic.retention.ms = 259200000
	topic.roll.ms = 14400000
	type = kafka
 (io.confluent.telemetry.exporter.kafka.KafkaExporterConfig)
[2022-10-10 14:46:47,714] INFO Initializing the event logger (io.confluent.telemetry.reporter.TelemetryReporter)
[2022-10-10 14:46:47,716] INFO EventLoggerConfig values: 
	event.logger.cloudevent.codec = structured
	event.logger.exporter.class = class io.confluent.shaded.io.confluent.telemetry.events.exporter.http.EventHttpExporter
 (io.confluent.shaded.io.confluent.telemetry.events.EventLoggerConfig)
[2022-10-10 14:46:47,721] INFO HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = http
 (io.confluent.shaded.io.confluent.telemetry.events.exporter.http.HttpExporterConfig)
[2022-10-10 14:46:48,075] INFO Creating kafka exporter named '_local' (io.confluent.telemetry.reporter.TelemetryReporter)
[2022-10-10 14:46:48,082] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9093]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.telemetry.serde.OpencensusMetricsProto
 (org.apache.kafka.clients.producer.ProducerConfig)
[2022-10-10 14:46:48,095] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:46:48,095] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:46:48,095] INFO Kafka startTimeMs: 1665406008095 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:46:48,096] INFO Starting Confluent telemetry reporter with an interval of 60000 ms for resource = (type = kafka) (io.confluent.telemetry.reporter.TelemetryReporter)
[2022-10-10 14:46:48,154] INFO [RaftManager nodeId=2] Registered the listener kafka.server.metadata.BrokerMetadataListener@275920282 (org.apache.kafka.raft.KafkaRaftClient)
[2022-10-10 14:46:48,155] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Starting (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:46:48,156] INFO [BrokerLifecycleManager id=2] Incarnation xv8pjXjmQXK3cCCpmUWbmg of broker 2 in cluster F1RidntJSxmR7lN5ArMZ8g is now STARTING. (kafka.server.BrokerLifecycleManager)
[2022-10-10 14:46:48,174] INFO [ExpirationReaper-2-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2022-10-10 14:46:48,175] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.samples.per.min = 0
 (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
[2022-10-10 14:46:48,175] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
 (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
[2022-10-10 14:46:48,175] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.samples.per.min = 0
 (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
[2022-10-10 14:46:48,175] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
 (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
[2022-10-10 14:46:48,175] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.samples.per.min = 0
 (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
[2022-10-10 14:46:48,176] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
 (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
[2022-10-10 14:46:48,176] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.samples.per.min = 0
 (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
[2022-10-10 14:46:48,176] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
 (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
[2022-10-10 14:46:48,176] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.samples.per.min = 0
 (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
[2022-10-10 14:46:48,176] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
 (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
[2022-10-10 14:46:48,176] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.samples.per.min = 0
 (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
[2022-10-10 14:46:48,176] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
 (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
[2022-10-10 14:46:48,176] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.samples.per.min = 0
 (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
[2022-10-10 14:46:48,176] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
 (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
[2022-10-10 14:46:48,177] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.samples.per.min = 0
 (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
[2022-10-10 14:46:48,177] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
 (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
[2022-10-10 14:46:48,374] INFO [RaftManager nodeId=2] Completed transition to CandidateState(localId=2, epoch=1, retries=1, electionTimeoutMs=1413) (org.apache.kafka.raft.QuorumState)
[2022-10-10 14:46:48,409] INFO [RaftManager nodeId=2] Completed transition to Leader(localId=2, epoch=1, epochStartOffset=0, highWatermark=Optional.empty, voterStates={1=ReplicaState(nodeId=1, endOffset=Optional.empty, lastFetchTimestamp=OptionalLong.empty, hasAcknowledgedLeader=false), 2=ReplicaState(nodeId=2, endOffset=Optional.empty, lastFetchTimestamp=OptionalLong.empty, hasAcknowledgedLeader=true), 3=ReplicaState(nodeId=3, endOffset=Optional.empty, lastFetchTimestamp=OptionalLong.empty, hasAcknowledgedLeader=false)}) (org.apache.kafka.raft.QuorumState)
[2022-10-10 14:46:48,410] INFO [Controller 2] Becoming the active controller at epoch 1, committed offset -1 and committed epoch -1. (org.apache.kafka.controller.QuorumController)
[2022-10-10 14:46:48,410] INFO [BrokerToControllerChannelManager broker=2 name=alterIsr]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:46:48,458] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:46:48,470] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:46:48,495] INFO [Controller 2] Registered new broker: RegisterBrokerRecord(brokerId=1, incarnationId=fnLHGMn_R2GUG2WL8-DVbQ, brokerEpoch=0, endPoints=[BrokerEndpoint(name='PLAINTEXT', host='localhost', port=9092, securityProtocol=0)], features=[], rack=null, fenced=true) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:46:48,495] INFO [Controller 2] Registered new broker: RegisterBrokerRecord(brokerId=2, incarnationId=xv8pjXjmQXK3cCCpmUWbmg, brokerEpoch=2, endPoints=[BrokerEndpoint(name='PLAINTEXT', host='localhost', port=9093, securityProtocol=0)], features=[], rack=null, fenced=true) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:46:48,568] INFO [Controller 2] Registered new broker: RegisterBrokerRecord(brokerId=3, incarnationId=UlWw6dT5TA6dgf33vsu76A, brokerEpoch=3, endPoints=[BrokerEndpoint(name='PLAINTEXT', host='localhost', port=9094, securityProtocol=0)], features=[], rack=null, fenced=true) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:46:48,641] INFO [BrokerLifecycleManager id=2] Successfully registered broker 2 with broker epoch 2 (kafka.server.BrokerLifecycleManager)
[2022-10-10 14:46:48,664] INFO [BrokerLifecycleManager id=2] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2022-10-10 14:46:48,676] INFO [BrokerServer id=2] Registering SBC on the broker's publisher (kafka.server.BrokerServer)
[2022-10-10 14:46:48,677] INFO [BrokerMetadataListener id=2] Starting to publish metadata events at offset 3. (kafka.server.metadata.BrokerMetadataListener)
[2022-10-10 14:46:48,678] INFO [BrokerMetadataPublisher id=2] Publishing initial metadata at offset OffsetAndEpoch(offset=3, epoch=1). (kafka.server.metadata.BrokerMetadataPublisher)
[2022-10-10 14:46:48,678] INFO Loading logs from log dirs ArraySeq(/tmp/server2/logs) (kafka.log.LogManager)
[2022-10-10 14:46:48,679] INFO Attempting recovery for all logs in /tmp/server2/logs since no clean shutdown file was found (kafka.log.LogManager)
[2022-10-10 14:46:48,681] INFO Loaded 0 logs in 2ms. (kafka.log.LogManager)
[2022-10-10 14:46:48,681] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2022-10-10 14:46:48,681] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2022-10-10 14:46:48,688] INFO [BrokerLifecycleManager id=2] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2022-10-10 14:46:48,718] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2022-10-10 14:46:48,719] INFO [GroupCoordinator 2]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2022-10-10 14:46:48,719] INFO [GroupCoordinator 2]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2022-10-10 14:46:48,720] INFO [TransactionCoordinator id=2] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2022-10-10 14:46:48,720] INFO [Transaction Marker Channel Manager 2]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2022-10-10 14:46:48,720] INFO [TransactionCoordinator id=2] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2022-10-10 14:46:48,725] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://localhost:9093
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 2
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = ucxex-roTpKFD1r9JRvnNw
	client.quota.callback.class = null
	compression.type = producer
	confluent.ansible.managed = false
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.persistence.store.startup.max.retry.ms = 300000
	confluent.balancer.api.state.persistence.store.startup.retry.wait.ms = 100
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.enable = false
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.replication.factor = 3
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.external.network.mititgation.enabled = false
	confluent.broker.health.manager.external.network.sample.duration.ms = 1000
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.healthy = 120
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.num.samples.before.network.unhealthy = 120
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.cdc.api.keys.load.timeout.ms = 600000000000
	confluent.cdc.api.keys.topic = 
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.enable = false
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 3
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.connection.invalid.request.delay.enable = false
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.partition.pruning.enable = true
	confluent.group.metadata.load.threads = 32
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.log.placement.constraints = 
	confluent.max.connection.creation.rate.per.ip = 2147483647
	confluent.max.connection.throttle.ms = null
	confluent.metadata.active.encryptor = null
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.max.partitions.per.request = 2147483647
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.offsets.topic.placement.constraints = 
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.prefer.tier.fetch.ms = -1
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.require.compatible.keystore.updates = true
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config = 
	confluent.segment.speculative.prefetch.enable = false
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.storage.probe.period.ms = -1
	confluent.telemetry.enabled = false
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.topic.replica.assignor.builder.class = 
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints = 
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@localhost:19092, 2@localhost:19093, 3@localhost:19094]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 3
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.2-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://:9093,CONTROLLER://:19093
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/server2/logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 2
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 2
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker, controller]
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2022-10-10 14:46:48,730] INFO [SocketServer listenerType=BROKER, nodeId=2] Starting socket server acceptors and processors (kafka.network.SocketServer)
[2022-10-10 14:46:48,731] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 3. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 14:46:48,731] INFO [SocketServer listenerType=BROKER, nodeId=2] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2022-10-10 14:46:48,731] INFO [SocketServer listenerType=BROKER, nodeId=2] Started socket server acceptors and processors (kafka.network.SocketServer)
[2022-10-10 14:46:48,733] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:46:48,744] INFO KafkaHttpServerConfig values: 
	access.control.allow.headers = 
	access.control.allow.methods = 
	access.control.allow.origin = 
	access.control.skip.options = true
	authentication.method = NONE
	authentication.realm = 
	authentication.roles = [*]
	authentication.skip.paths = []
	compression.enable = true
	csrf.prevention.enable = false
	csrf.prevention.token.endpoint = /csrf
	csrf.prevention.token.expiration.minutes = 30
	csrf.prevention.token.max.entries = 10000
	debug = false
	dos.filter.delay.ms = 100
	dos.filter.enabled = false
	dos.filter.insert.headers = true
	dos.filter.ip.whitelist = []
	dos.filter.managed.attr = false
	dos.filter.max.idle.tracker.ms = 30000
	dos.filter.max.requests.ms = 30000
	dos.filter.max.requests.per.connection.per.sec = 25
	dos.filter.max.requests.per.sec = 25
	dos.filter.max.wait.ms = 50
	dos.filter.throttle.ms = 30000
	dos.filter.throttled.requests = 5
	http2.enabled = true
	idle.timeout.ms = 30000
	listener.protocol.map = []
	listeners = [http://0.0.0.0:8090]
	metric.reporters = []
	metrics.jmx.prefix = rest-utils
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	metrics.tag.map = []
	nosniff.prevention.enable = false
	port = 8090
	proxy.protocol.enabled = false
	reject.options.request = false
	request.logger.name = io.confluent.rest-utils.requests
	request.queue.capacity = 2147483647
	request.queue.capacity.growby = 64
	request.queue.capacity.init = 128
	resource.extension.classes = []
	response.http.headers.config = 
	response.mediatype.default = application/json
	response.mediatype.preferred = [application/json]
	rest.servlet.initializor.classes = []
	shutdown.graceful.ms = 1000
	ssl.cipher.suites = []
	ssl.client.auth = false
	ssl.client.authentication = NONE
	ssl.enabled.protocols = []
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = 
	ssl.keystore.location = 
	ssl.keystore.password = [hidden]
	ssl.keystore.reload = false
	ssl.keystore.type = JKS
	ssl.keystore.watch.location = 
	ssl.protocol = TLS
	ssl.provider = 
	ssl.trustmanager.algorithm = 
	ssl.truststore.location = 
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	thread.pool.max = 200
	thread.pool.min = 8
	websocket.path.prefix = /ws
	websocket.servlet.initializor.classes = []
 (io.confluent.http.server.KafkaHttpServerConfig)
[2022-10-10 14:46:48,776] INFO Application provider 'MetadataApiApplicationProvider' provided 1 instance(s). (io.confluent.http.server.KafkaHttpApplicationLoader)
[2022-10-10 14:46:48,782] INFO Application provider 'RbacApplicationProvider' did not provide any instances. (io.confluent.http.server.KafkaHttpApplicationLoader)
[2022-10-10 14:46:48,801] INFO KafkaHttpServerConfig values: 
	access.control.allow.headers = 
	access.control.allow.methods = 
	access.control.allow.origin = 
	access.control.skip.options = true
	authentication.method = NONE
	authentication.realm = 
	authentication.roles = [*]
	authentication.skip.paths = []
	compression.enable = true
	csrf.prevention.enable = false
	csrf.prevention.token.endpoint = /csrf
	csrf.prevention.token.expiration.minutes = 30
	csrf.prevention.token.max.entries = 10000
	debug = false
	dos.filter.delay.ms = 100
	dos.filter.enabled = false
	dos.filter.insert.headers = true
	dos.filter.ip.whitelist = []
	dos.filter.managed.attr = false
	dos.filter.max.idle.tracker.ms = 30000
	dos.filter.max.requests.ms = 30000
	dos.filter.max.requests.per.connection.per.sec = 25
	dos.filter.max.requests.per.sec = 25
	dos.filter.max.wait.ms = 50
	dos.filter.throttle.ms = 30000
	dos.filter.throttled.requests = 5
	http2.enabled = true
	idle.timeout.ms = 30000
	listener.protocol.map = []
	listeners = [http://0.0.0.0:8090]
	metric.reporters = []
	metrics.jmx.prefix = rest-utils
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	metrics.tag.map = []
	nosniff.prevention.enable = false
	port = 8090
	proxy.protocol.enabled = false
	reject.options.request = false
	request.logger.name = io.confluent.rest-utils.requests
	request.queue.capacity = 2147483647
	request.queue.capacity.growby = 64
	request.queue.capacity.init = 128
	resource.extension.classes = []
	response.http.headers.config = 
	response.mediatype.default = application/json
	response.mediatype.preferred = [application/json]
	rest.servlet.initializor.classes = []
	shutdown.graceful.ms = 1000
	ssl.cipher.suites = []
	ssl.client.auth = false
	ssl.client.authentication = NONE
	ssl.enabled.protocols = []
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = 
	ssl.keystore.location = 
	ssl.keystore.password = [hidden]
	ssl.keystore.reload = false
	ssl.keystore.type = JKS
	ssl.keystore.watch.location = 
	ssl.protocol = TLS
	ssl.provider = 
	ssl.trustmanager.algorithm = 
	ssl.truststore.location = 
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	thread.pool.max = 200
	thread.pool.min = 8
	websocket.path.prefix = /ws
	websocket.servlet.initializor.classes = []
 (io.confluent.http.server.KafkaHttpServerConfig)
[2022-10-10 14:46:48,803] INFO [BrokerLifecycleManager id=2] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2022-10-10 14:46:48,805] INFO KafkaHttpServerConfig values: 
	access.control.allow.headers = 
	access.control.allow.methods = 
	access.control.allow.origin = 
	access.control.skip.options = true
	authentication.method = NONE
	authentication.realm = 
	authentication.roles = [*]
	authentication.skip.paths = []
	compression.enable = true
	csrf.prevention.enable = false
	csrf.prevention.token.endpoint = /csrf
	csrf.prevention.token.expiration.minutes = 30
	csrf.prevention.token.max.entries = 10000
	debug = false
	dos.filter.delay.ms = 100
	dos.filter.enabled = false
	dos.filter.insert.headers = true
	dos.filter.ip.whitelist = []
	dos.filter.managed.attr = false
	dos.filter.max.idle.tracker.ms = 30000
	dos.filter.max.requests.ms = 30000
	dos.filter.max.requests.per.connection.per.sec = 25
	dos.filter.max.requests.per.sec = 25
	dos.filter.max.wait.ms = 50
	dos.filter.throttle.ms = 30000
	dos.filter.throttled.requests = 5
	http2.enabled = true
	idle.timeout.ms = 30000
	listener.protocol.map = []
	listeners = [http://0.0.0.0:8090]
	metric.reporters = []
	metrics.jmx.prefix = rest-utils
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	metrics.tag.map = []
	nosniff.prevention.enable = false
	port = 8090
	proxy.protocol.enabled = false
	reject.options.request = false
	request.logger.name = io.confluent.rest-utils.requests
	request.queue.capacity = 2147483647
	request.queue.capacity.growby = 64
	request.queue.capacity.init = 128
	resource.extension.classes = []
	response.http.headers.config = 
	response.mediatype.default = application/json
	response.mediatype.preferred = [application/json]
	rest.servlet.initializor.classes = []
	shutdown.graceful.ms = 1000
	ssl.cipher.suites = []
	ssl.client.auth = false
	ssl.client.authentication = NONE
	ssl.enabled.protocols = []
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = 
	ssl.keystore.location = 
	ssl.keystore.password = [hidden]
	ssl.keystore.reload = false
	ssl.keystore.type = JKS
	ssl.keystore.watch.location = 
	ssl.protocol = TLS
	ssl.provider = 
	ssl.trustmanager.algorithm = 
	ssl.truststore.location = 
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	thread.pool.max = 200
	thread.pool.min = 8
	websocket.path.prefix = /ws
	websocket.servlet.initializor.classes = []
 (io.confluent.http.server.KafkaHttpServerConfig)
[2022-10-10 14:46:48,805] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://localhost:9093
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 2
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = ucxex-roTpKFD1r9JRvnNw
	client.quota.callback.class = null
	compression.type = producer
	confluent.ansible.managed = false
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.persistence.store.startup.max.retry.ms = 300000
	confluent.balancer.api.state.persistence.store.startup.retry.wait.ms = 100
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.enable = false
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.replication.factor = 3
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.external.network.mititgation.enabled = false
	confluent.broker.health.manager.external.network.sample.duration.ms = 1000
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.healthy = 120
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.num.samples.before.network.unhealthy = 120
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.cdc.api.keys.load.timeout.ms = 600000000000
	confluent.cdc.api.keys.topic = 
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.enable = false
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 3
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.connection.invalid.request.delay.enable = false
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.partition.pruning.enable = true
	confluent.group.metadata.load.threads = 32
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.log.placement.constraints = 
	confluent.max.connection.creation.rate.per.ip = 2147483647
	confluent.max.connection.throttle.ms = null
	confluent.metadata.active.encryptor = null
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.max.partitions.per.request = 2147483647
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.offsets.topic.placement.constraints = 
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.prefer.tier.fetch.ms = -1
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.require.compatible.keystore.updates = true
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config = 
	confluent.segment.speculative.prefetch.enable = false
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.storage.probe.period.ms = -1
	confluent.telemetry.enabled = false
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.topic.replica.assignor.builder.class = 
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints = 
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@localhost:19092, 2@localhost:19093, 3@localhost:19094]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 3
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.2-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://:9093,CONTROLLER://:19093
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/server2/logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 2
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 2
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker, controller]
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2022-10-10 14:46:48,812] INFO KafkaRestConfig values: 
	access.control.allow.headers = 
	access.control.allow.methods = 
	access.control.allow.origin = 
	access.control.skip.options = true
	advertised.listeners = []
	api.endpoints.allowlist = []
	api.endpoints.blocklist = []
	api.v2.enable = false
	api.v3.enable = true
	api.v3.produce.rate.limit.cache.expiry.ms = 3600000
	api.v3.produce.rate.limit.enabled = false
	api.v3.produce.rate.limit.max.bytes.global.per.sec = 10000000
	api.v3.produce.rate.limit.max.bytes.per.sec = 10000000
	api.v3.produce.rate.limit.max.requests.global.per.sec = 10000
	api.v3.produce.rate.limit.max.requests.per.sec = 10000
	api.v3.produce.response.thread.pool.size = 8
	authentication.method = NONE
	authentication.realm = 
	authentication.roles = [*]
	authentication.skip.paths = []
	bootstrap.servers = localhost:9093
	client.init.timeout.ms = 60000
	client.sasl.kerberos.kinit.cmd = /usr/bin/kinit
	client.sasl.kerberos.min.time.before.relogin = 60000
	client.sasl.kerberos.service.name = 
	client.sasl.kerberos.ticket.renew.jitter = 0.05
	client.sasl.kerberos.ticket.renew.window.factor = 0.8
	client.sasl.mechanism = GSSAPI
	client.security.protocol = PLAINTEXT
	client.ssl.cipher.suites = 
	client.ssl.enabled.protocols = TLSv1.2,TLSv1.1,TLSv1
	client.ssl.endpoint.identification.algorithm = 
	client.ssl.key.password = [hidden]
	client.ssl.keymanager.algorithm = SunX509
	client.ssl.keystore.location = 
	client.ssl.keystore.password = [hidden]
	client.ssl.keystore.type = JKS
	client.ssl.protocol = TLS
	client.ssl.provider = 
	client.ssl.trustmanager.algorithm = PKIX
	client.ssl.truststore.location = 
	client.ssl.truststore.password = [hidden]
	client.ssl.truststore.type = JKS
	client.timeout.ms = 500
	client.zk.session.timeout.ms = 30000
	compression.enable = true
	confluent.resource.name.authority = 
	consumer.instance.timeout.ms = 300000
	consumer.iterator.backoff.ms = 50
	consumer.iterator.timeout.ms = 1
	consumer.request.max.bytes = 67108864
	consumer.request.timeout.ms = 1000
	consumer.threads = 50
	csrf.prevention.enable = false
	csrf.prevention.token.endpoint = /csrf
	csrf.prevention.token.expiration.minutes = 30
	csrf.prevention.token.max.entries = 10000
	debug = false
	dos.filter.delay.ms = 100
	dos.filter.enabled = false
	dos.filter.insert.headers = true
	dos.filter.ip.whitelist = []
	dos.filter.managed.attr = false
	dos.filter.max.idle.tracker.ms = 30000
	dos.filter.max.requests.ms = 30000
	dos.filter.max.requests.per.connection.per.sec = 25
	dos.filter.max.requests.per.sec = 25
	dos.filter.max.wait.ms = 50
	dos.filter.throttle.ms = 30000
	dos.filter.throttled.requests = 5
	fetch.min.bytes = -1
	host.name = 
	http2.enabled = true
	id = 
	idle.timeout.ms = 30000
	kafka.rest.resource.extension.class = [io.confluent.kafkarest.KafkaRestResourceExtension]
	listener.protocol.map = []
	listeners = []
	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
	metrics.jmx.prefix = kafka.rest
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	metrics.tag.map = []
	nosniff.prevention.enable = false
	port = 8082
	producer.threads = 5
	proxy.protocol.enabled = false
	rate.limit.backend = guava
	rate.limit.costs = 
	rate.limit.default.cost = 1
	rate.limit.enable = false
	rate.limit.permits.per.sec = 50
	rate.limit.timeout.ms = 0
	reject.options.request = false
	request.logger.name = io.confluent.rest-utils.requests
	request.queue.capacity = 2147483647
	request.queue.capacity.growby = 64
	request.queue.capacity.init = 128
	resource.extension.classes = []
	response.http.headers.config = 
	response.mediatype.default = application/json
	response.mediatype.preferred = [application/json, application/vnd.kafka.v2+json]
	rest.servlet.initializor.classes = []
	schema.registry.url = http://localhost:8081
	shutdown.graceful.ms = 1000
	simpleconsumer.pool.size.max = 25
	simpleconsumer.pool.timeout.ms = 1000
	ssl.cipher.suites = []
	ssl.client.auth = false
	ssl.client.authentication = NONE
	ssl.enabled.protocols = []
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = 
	ssl.keystore.location = 
	ssl.keystore.password = [hidden]
	ssl.keystore.reload = false
	ssl.keystore.type = JKS
	ssl.keystore.watch.location = 
	ssl.protocol = TLS
	ssl.provider = 
	ssl.trustmanager.algorithm = 
	ssl.truststore.location = 
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	thread.pool.max = 200
	thread.pool.min = 8
	websocket.path.prefix = /ws
	websocket.servlet.initializor.classes = []
	zookeeper.connect = 
 (io.confluent.kafkarest.KafkaRestConfig)
[2022-10-10 14:46:48,812] INFO EventEmitterConfig values: 
 (io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig)
[2022-10-10 14:46:48,812] INFO EventEmitterConfig values: 
 (io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig)
[2022-10-10 14:46:48,813] INFO ConfluentTelemetryConfig values: 
	confluent.telemetry.api.key = null
	confluent.telemetry.api.secret = null
	confluent.telemetry.debug.enabled = false
	confluent.telemetry.enabled = false
	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
	confluent.telemetry.events.enable = true
	confluent.telemetry.metrics.collector.include = .*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|committed|used).*
	confluent.telemetry.metrics.collector.interval.ms = 60000
	confluent.telemetry.metrics.collector.slo.enabled = false
	confluent.telemetry.proxy.password = null
	confluent.telemetry.proxy.url = null
	confluent.telemetry.proxy.username = null
 (io.confluent.telemetry.ConfluentTelemetryConfig)
[2022-10-10 14:46:48,813] INFO VolumeMetricsCollectorConfig values: 
	confluent.telemetry.metrics.collector.volume.update.ms = 15000
 (io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig)
[2022-10-10 14:46:48,813] INFO HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	metrics.include = .*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|committed|used).*
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = http
 (io.confluent.telemetry.exporter.http.HttpExporterConfig)
[2022-10-10 14:46:48,813] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig)
[2022-10-10 14:46:48,813] WARN Ignoring redefinition of existing telemetry label kafka_rest.version (io.confluent.shaded.io.confluent.telemetry.ResourceBuilderFacade)
[2022-10-10 14:46:48,813] INFO ConfluentTelemetryConfig values: 
	confluent.telemetry.api.key = null
	confluent.telemetry.api.secret = null
	confluent.telemetry.debug.enabled = false
	confluent.telemetry.enabled = false
	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
	confluent.telemetry.events.enable = true
	confluent.telemetry.metrics.collector.include = .*io.confluent.telemetry/.*.*|.*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|committed|used).*|.*io.confluent.kafka.rest/.*(connections_active|connections_closed_rate|request_error_rate|request_latency_avg|request_latency_max|request_rate|response_size_avg|response_size_max).*
	confluent.telemetry.metrics.collector.interval.ms = 60000
	confluent.telemetry.metrics.collector.slo.enabled = false
	confluent.telemetry.proxy.password = null
	confluent.telemetry.proxy.url = null
	confluent.telemetry.proxy.username = null
 (io.confluent.telemetry.ConfluentTelemetryConfig)
[2022-10-10 14:46:48,813] INFO VolumeMetricsCollectorConfig values: 
	confluent.telemetry.metrics.collector.volume.update.ms = 15000
 (io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig)
[2022-10-10 14:46:48,813] INFO HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	metrics.include = .*io.confluent.telemetry/.*.*|.*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|committed|used).*|.*io.confluent.kafka.rest/.*(connections_active|connections_closed_rate|request_error_rate|request_latency_avg|request_latency_max|request_rate|response_size_avg|response_size_max).*
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = http
 (io.confluent.telemetry.exporter.http.HttpExporterConfig)
[2022-10-10 14:46:48,813] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig)
[2022-10-10 14:46:48,813] INFO Initializing the event logger (io.confluent.telemetry.reporter.TelemetryReporter)
[2022-10-10 14:46:48,814] INFO EventLoggerConfig values: 
	event.logger.cloudevent.codec = structured
	event.logger.exporter.class = class io.confluent.shaded.io.confluent.telemetry.events.exporter.http.EventHttpExporter
 (io.confluent.shaded.io.confluent.telemetry.events.EventLoggerConfig)
[2022-10-10 14:46:48,814] INFO HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = http
 (io.confluent.shaded.io.confluent.telemetry.events.exporter.http.HttpExporterConfig)
[2022-10-10 14:46:48,816] INFO Starting Confluent telemetry reporter with an interval of 60000 ms for resource = (type = kafka_rest) (io.confluent.telemetry.reporter.TelemetryReporter)
[2022-10-10 14:46:48,817] INFO Application provider 'KafkaRestApplicationProvider' provided 1 instance(s). (io.confluent.http.server.KafkaHttpApplicationLoader)
[2022-10-10 14:46:48,867] INFO Loaded KafkaHttpServer implementation class io.confluent.http.server.KafkaHttpServerImpl (io.confluent.kafka.http.server.KafkaHttpServerLoader)
[2022-10-10 14:46:48,867] INFO KafkaHttpServer transitioned from NEW to STARTING.. (io.confluent.http.server.KafkaHttpServerImpl)
[2022-10-10 14:46:48,998] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
[2022-10-10 14:46:48,998] INFO SchemaRegistryConfig values: 
	auto.register.schemas = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	use.latest.version = false
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
 (io.confluent.kafkarest.config.SchemaRegistryConfig)
Oct 10, 2022 2:46:49 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider io.confluent.metadataapi.resources.MetadataResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.metadataapi.resources.MetadataResource will be ignored. 
[2022-10-10 14:46:49,238] INFO HV000001: Hibernate Validator 6.1.7.Final (org.hibernate.validator.internal.util.Version)
[2022-10-10 14:46:49,248] INFO [Producer clientId=confluent-telemetry-reporter-local-producer] Cluster ID: F1RidntJSxmR7lN5ArMZ8g (org.apache.kafka.clients.Metadata)
[2022-10-10 14:46:49,387] INFO [Controller 2] The request from broker 1 to unfence has been granted because it has caught up with the last committed metadata offset 4. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 14:46:49,387] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=1, epoch=0) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:46:49,416] INFO [Controller 2] The request from broker 3 to unfence has been granted because it has caught up with the last committed metadata offset 4. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 14:46:49,416] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=3, epoch=3) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:46:49,418] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
[2022-10-10 14:46:49,438] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
[2022-10-10 14:46:49,460] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
[2022-10-10 14:46:49,488] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
[2022-10-10 14:46:49,848] INFO KafkaHttpServer transitioned from STARTING to RUNNING.. (io.confluent.http.server.KafkaHttpServerImpl)
[2022-10-10 14:46:49,864] INFO [BrokerServer id=2] Skipping durability audit instantiation (kafka.server.BrokerServer)
[2022-10-10 14:46:49,865] INFO LicenseConfig values: 
	confluent.license = [hidden]
	confluent.license.retry.backoff.max.ms = 100000
	confluent.license.retry.backoff.min.ms = 1000
	confluent.license.topic = _confluent-license
	confluent.license.topic.create.timeout.ms = 600000
	confluent.license.topic.replication.factor = 3
 (io.confluent.license.validator.LicenseConfig)
[2022-10-10 14:46:49,866] INFO LicenseConfig values: 
	confluent.license = [hidden]
	confluent.license.retry.backoff.max.ms = 100000
	confluent.license.retry.backoff.min.ms = 1000
	confluent.license.topic = _confluent-license
	confluent.license.topic.create.timeout.ms = 600000
	confluent.license.topic.replication.factor = 3
 (io.confluent.license.validator.LicenseConfig)
[2022-10-10 14:46:49,881] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-admin-2
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:46:49,897] WARN The configuration 'replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:46:49,897] WARN The configuration 'min.insync.replicas' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:46:49,897] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:46:49,897] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:46:49,897] INFO Kafka startTimeMs: 1665406009897 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:46:49,923] INFO App info kafka.admin.client for _confluent-license-admin-2 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:46:49,925] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:46:49,925] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:46:49,925] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:46:49,931] INFO Starting License Store (io.confluent.license.LicenseStore)
[2022-10-10 14:46:49,931] INFO Starting KafkaBasedLog with topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog)
[2022-10-10 14:46:49,931] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-admin-2
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:46:49,932] WARN The configuration 'replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:46:49,932] WARN The configuration 'min.insync.replicas' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:46:49,932] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:46:49,932] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:46:49,932] INFO Kafka startTimeMs: 1665406009932 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:46:49,972] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='min.insync.replicas', value='2')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:46:49,972] INFO [Controller 2] ConfigResource(type=TOPIC, name='_confluent-command'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
[2022-10-10 14:46:49,972] INFO [Controller 2] ConfigResource(type=TOPIC, name='_confluent-command'): set configuration min.insync.replicas to 2 (org.apache.kafka.controller.ConfigurationControlManager)
[2022-10-10 14:46:49,973] INFO [Controller 2] Created topic _confluent-command with topic ID 6kDRuMk5RnWBjO6yAEuf4Q. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:46:49,973] INFO [Controller 2] Created partition _confluent-command-0 with topic ID 6kDRuMk5RnWBjO6yAEuf4Q and PartitionRegistration(replicas=[3, 1, 2], isr=[3, 1, 2], removingReplicas=[], addingReplicas=[], leader=3, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:46:50,066] INFO App info kafka.admin.client for _confluent-license-admin-2 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:46:50,073] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:46:50,073] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:46:50,073] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:46:50,077] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9093]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-consumer-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class io.confluent.license.LicenseStore$LicenseKeySerde
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
 (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-10-10 14:46:50,090] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:46:50,090] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:46:50,090] INFO Kafka startTimeMs: 1665406010090 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:46:50,097] INFO [Consumer clientId=_confluent-license-consumer-2, groupId=null] Cluster ID: F1RidntJSxmR7lN5ArMZ8g (org.apache.kafka.clients.Metadata)
[2022-10-10 14:46:50,102] INFO [MergedLog partition=_confluent-command-0, dir=/tmp/server2/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2022-10-10 14:46:50,105] INFO Created log for partition _confluent-command-0 in /tmp/server2/logs/_confluent-command-0 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
[2022-10-10 14:46:50,105] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:46:50,105] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:46:50,105] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:46:50,106] INFO App info kafka.consumer for _confluent-license-consumer-2 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:46:50,106] INFO [Partition _confluent-command-0 broker=2] No checkpointed highwatermark is found for partition _confluent-command-0 (kafka.cluster.Partition)
[2022-10-10 14:46:50,107] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9093]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class io.confluent.license.LicenseStore$LicenseKeySerde
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
 (org.apache.kafka.clients.producer.ProducerConfig)
[2022-10-10 14:46:50,107] INFO [Partition _confluent-command-0 broker=2] Log loaded for partition _confluent-command-0 with initial high watermark 0 (kafka.cluster.Partition)
[2022-10-10 14:46:50,108] INFO Setting topicIdPartition 6kDRuMk5RnWBjO6yAEuf4Q:_confluent-command-0 (kafka.tier.state.FileTierPartitionState)
[2022-10-10 14:46:50,108] INFO [MergedLog partition=_confluent-command-0, dir=/tmp/server2/logs] Initializing tier metadata without recovery for _confluent-command-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2022-10-10 14:46:50,110] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:46:50,110] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:46:50,110] INFO Kafka startTimeMs: 1665406010110 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:46:50,110] INFO [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-command-0) (kafka.server.ReplicaFetcherManager)
[2022-10-10 14:46:50,111] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9093]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-consumer-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class io.confluent.license.LicenseStore$LicenseKeySerde
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
 (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-10-10 14:46:50,111] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:46:50,111] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:46:50,111] INFO Kafka startTimeMs: 1665406010111 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:46:50,116] INFO [Producer clientId=_confluent-license-producer-2] Cluster ID: F1RidntJSxmR7lN5ArMZ8g (org.apache.kafka.clients.Metadata)
[2022-10-10 14:46:50,117] INFO [Consumer clientId=_confluent-license-consumer-2, groupId=null] Cluster ID: F1RidntJSxmR7lN5ArMZ8g (org.apache.kafka.clients.Metadata)
[2022-10-10 14:46:50,118] INFO [Consumer clientId=_confluent-license-consumer-2, groupId=null] Subscribed to partition(s): _confluent-command-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2022-10-10 14:46:50,121] INFO [Consumer clientId=_confluent-license-consumer-2, groupId=null] Seeking to EARLIEST offset of partition _confluent-command-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-10-10 14:46:50,121] INFO [ReplicaFetcher replicaId=2, leaderId=3, fetcherId=0] Starting (kafka.server.ReplicaFetcherThread)
[2022-10-10 14:46:50,124] INFO [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-3-Default to broker 3 for partitions HashMap(_confluent-command-0 -> InitialFetchState(Some(6kDRuMk5RnWBjO6yAEuf4Q),BrokerEndPoint(id=3, host=localhost:9094),0,0)) (kafka.server.ReplicaFetcherManager)
[2022-10-10 14:46:50,125] INFO [ReplicaFetcher replicaId=2, leaderId=3, fetcherId=0] Truncating partition _confluent-command-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2022-10-10 14:46:50,126] INFO [MergedLog partition=_confluent-command-0, dir=/tmp/server2/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.MergedLog)
[2022-10-10 14:46:50,142] WARN [Consumer clientId=_confluent-license-consumer-2, groupId=null] Error while fetching metadata with correlation id 4 : {_confluent-command=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:46:50,146] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=3, assignments=[], configs=[], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-command' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:46:50,149] INFO [BrokerMetadataPublisher id=2] Updating topic _confluent-command with new configuration : cleanup.policy -> compact,min.insync.replicas -> 2 (kafka.server.metadata.BrokerMetadataPublisher)
[2022-10-10 14:46:50,151] INFO Setting tieringEnabled to false (earlier value: false) (kafka.tier.state.FileTierPartitionState)
[2022-10-10 14:46:50,166] WARN [ReplicaFetcher replicaId=2, leaderId=3, fetcherId=0] Received UNKNOWN_TOPIC_ID from the leader for partition _confluent-command-0. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist. (kafka.server.ReplicaFetcherThread)
[2022-10-10 14:46:50,245] WARN [Consumer clientId=_confluent-license-consumer-2, groupId=null] Error while fetching metadata with correlation id 5 : {_confluent-command=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:46:50,246] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=3, assignments=[], configs=[], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-command' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:46:50,349] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=3, assignments=[], configs=[], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-command' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:46:50,349] WARN [Consumer clientId=_confluent-license-consumer-2, groupId=null] Error while fetching metadata with correlation id 6 : {_confluent-command=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:46:50,453] WARN [Consumer clientId=_confluent-license-consumer-2, groupId=null] Error while fetching metadata with correlation id 7 : {_confluent-command=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:46:50,453] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=3, assignments=[], configs=[], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-command' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:46:50,493] INFO [Controller 2] Unfenced broker 1 has requested and been granted an immediate shutdown. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 14:46:50,495] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:46:50,495] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=1, epoch=0) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:46:50,506] INFO [Controller 2] Unfenced broker 3 has requested and been granted a controlled shutdown. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 14:46:50,507] INFO [Controller 2] enterControlledShutdown[3]: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:46:50,556] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=3, assignments=[], configs=[], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-command' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:46:50,558] WARN [Consumer clientId=_confluent-license-consumer-2, groupId=null] Error while fetching metadata with correlation id 8 : {_confluent-command=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:46:50,577] INFO [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-command-0) (kafka.server.ReplicaFetcherManager)
[2022-10-10 14:46:50,584] INFO [Consumer clientId=_confluent-license-consumer-2, groupId=null] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:46:50,585] INFO [ReplicaFetcher replicaId=2, leaderId=3, fetcherId=0] Shutting down (kafka.server.ReplicaFetcherThread)
[2022-10-10 14:46:50,585] INFO [ReplicaFetcher replicaId=2, leaderId=3, fetcherId=0] Stopped (kafka.server.ReplicaFetcherThread)
[2022-10-10 14:46:50,585] INFO [ReplicaFetcher replicaId=2, leaderId=3, fetcherId=0] Shutdown completed (kafka.server.ReplicaFetcherThread)
[2022-10-10 14:46:50,664] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=3, assignments=[], configs=[], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-command' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:46:50,664] WARN [Consumer clientId=_confluent-license-consumer-2, groupId=null] Error while fetching metadata with correlation id 10 : {_confluent-command=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:46:50,706] INFO [RaftManager nodeId=2] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:46:50,767] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=3, assignments=[], configs=[], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-command' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:46:50,767] WARN [Consumer clientId=_confluent-license-consumer-2, groupId=null] Error while fetching metadata with correlation id 11 : {_confluent-command=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:46:50,871] WARN [Consumer clientId=_confluent-license-consumer-2, groupId=null] Error while fetching metadata with correlation id 12 : {_confluent-command=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:46:50,871] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=3, assignments=[], configs=[], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-command' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:46:50,975] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=3, assignments=[], configs=[], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-command' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:46:50,975] WARN [Consumer clientId=_confluent-license-consumer-2, groupId=null] Error while fetching metadata with correlation id 13 : {_confluent-command=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:46:51,082] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=3, assignments=[], configs=[], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-command' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:46:51,082] WARN [Consumer clientId=_confluent-license-consumer-2, groupId=null] Error while fetching metadata with correlation id 14 : {_confluent-command=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:46:51,186] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=3, assignments=[], configs=[], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-command' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:46:51,187] WARN [Consumer clientId=_confluent-license-consumer-2, groupId=null] Error while fetching metadata with correlation id 15 : {_confluent-command=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:46:51,293] WARN [Consumer clientId=_confluent-license-consumer-2, groupId=null] Error while fetching metadata with correlation id 16 : {_confluent-command=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:46:51,293] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=3, assignments=[], configs=[], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-command' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:46:51,399] WARN [Consumer clientId=_confluent-license-consumer-2, groupId=null] Error while fetching metadata with correlation id 17 : {_confluent-command=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:46:51,399] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=3, assignments=[], configs=[], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-command' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:46:51,507] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=3, assignments=[], configs=[], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-command' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:46:51,507] WARN [Consumer clientId=_confluent-license-consumer-2, groupId=null] Error while fetching metadata with correlation id 18 : {_confluent-command=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:46:51,612] WARN [Consumer clientId=_confluent-license-consumer-2, groupId=null] Error while fetching metadata with correlation id 19 : {_confluent-command=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:46:51,612] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=3, assignments=[], configs=[], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-command' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:46:51,716] WARN [Consumer clientId=_confluent-license-consumer-2, groupId=null] Error while fetching metadata with correlation id 20 : {_confluent-command=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:46:51,717] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=3, assignments=[], configs=[], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-command' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:46:51,823] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=3, assignments=[], configs=[], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-command' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:46:51,823] WARN [Consumer clientId=_confluent-license-consumer-2, groupId=null] Error while fetching metadata with correlation id 21 : {_confluent-command=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:46:51,929] WARN [Consumer clientId=_confluent-license-consumer-2, groupId=null] Error while fetching metadata with correlation id 22 : {_confluent-command=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:46:51,929] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=3, assignments=[], configs=[], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-command' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:46:52,036] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=3, assignments=[], configs=[], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-command' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:46:52,036] WARN [Consumer clientId=_confluent-license-consumer-2, groupId=null] Error while fetching metadata with correlation id 23 : {_confluent-command=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:46:52,145] WARN [Consumer clientId=_confluent-license-consumer-2, groupId=null] Error while fetching metadata with correlation id 24 : {_confluent-command=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:46:52,145] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=3, assignments=[], configs=[], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-command' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:46:52,247] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=3, assignments=[], configs=[], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-command' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:46:52,247] WARN [Consumer clientId=_confluent-license-consumer-2, groupId=null] Error while fetching metadata with correlation id 25 : {_confluent-command=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:46:52,353] WARN [Consumer clientId=_confluent-license-consumer-2, groupId=null] Error while fetching metadata with correlation id 26 : {_confluent-command=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:46:52,353] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=3, assignments=[], configs=[], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-command' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:46:52,460] WARN [Consumer clientId=_confluent-license-consumer-2, groupId=null] Error while fetching metadata with correlation id 27 : {_confluent-command=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:46:52,460] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=3, assignments=[], configs=[], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-command' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:46:52,567] WARN [Consumer clientId=_confluent-license-consumer-2, groupId=null] Error while fetching metadata with correlation id 28 : {_confluent-command=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:46:52,567] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=3, assignments=[], configs=[], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-command' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:46:52,642] INFO [Controller 2] The request from broker 3 to shut down has been granted since the lowest active offset 13 is now greater than the broker's controlled shutdown offset 13. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 14:46:52,643] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=3, epoch=3) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:46:52,672] WARN [Consumer clientId=_confluent-license-consumer-2, groupId=null] Error while fetching metadata with correlation id 29 : {_confluent-command=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:46:52,672] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=3, assignments=[], configs=[], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-command' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:46:52,725] INFO [Consumer clientId=_confluent-license-consumer-2, groupId=null] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:46:52,775] INFO [Consumer clientId=_confluent-license-consumer-2, groupId=null] Resetting the last seen epoch of partition _confluent-command-0 to 2 since the associated topicId changed from null to 6kDRuMk5RnWBjO6yAEuf4Q (org.apache.kafka.clients.Metadata)
[2022-10-10 14:46:52,788] INFO Finished reading KafkaBasedLog for topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog)
[2022-10-10 14:46:52,788] INFO Started KafkaBasedLog for topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog)
[2022-10-10 14:46:52,788] INFO Started License Store (io.confluent.license.LicenseStore)
[2022-10-10 14:46:52,855] INFO [RaftManager nodeId=2] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:46:53,317] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-admin-2
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:46:53,318] WARN The configuration 'replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:46:53,318] WARN The configuration 'min.insync.replicas' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:46:53,318] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:46:53,318] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:46:53,318] INFO Kafka startTimeMs: 1665406013318 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:46:53,327] INFO App info kafka.admin.client for _confluent-license-admin-2 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:46:53,327] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:46:53,327] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:46:53,327] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:46:53,359] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-admin-2
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:46:53,359] WARN The configuration 'replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:46:53,359] WARN The configuration 'min.insync.replicas' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:46:53,359] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:46:53,359] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:46:53,360] INFO Kafka startTimeMs: 1665406013359 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:46:53,362] INFO App info kafka.admin.client for _confluent-license-admin-2 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:46:53,363] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:46:53,363] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:46:53,363] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:46:53,363] INFO License for single cluster, single node (io.confluent.license.LicenseManager)
[2022-10-10 14:46:53,365] INFO Starting delay for broker load metric (kafka.metrics.BrokerLoad)
[2022-10-10 14:46:53,366] INFO [BrokerServer id=2] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2022-10-10 14:46:53,366] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:46:53,366] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:46:53,366] INFO Kafka startTimeMs: 1665406013366 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:46:53,366] INFO Kafka Server started (kafka.server.KafkaRaftServer)
[2022-10-10 14:47:48,179] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:47:48,185] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:47:48,185] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:47:48,185] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:47:48,185] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:47:48,185] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:47:48,185] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:47:48,185] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:47:48,185] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:47:48,185] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:47:48,185] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:47:48,185] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:47:48,185] INFO Kafka startTimeMs: 1665406068185 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:47:48,200] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:47:48,200] INFO [Controller 2] ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics'): set configuration max.message.bytes to 10485760 (org.apache.kafka.controller.ConfigurationControlManager)
[2022-10-10 14:47:48,200] INFO [Controller 2] ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
[2022-10-10 14:47:48,200] INFO [Controller 2] ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
[2022-10-10 14:47:48,200] INFO [Controller 2] ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics'): set configuration retention.ms to 259200000 (org.apache.kafka.controller.ConfigurationControlManager)
[2022-10-10 14:47:48,200] INFO [Controller 2] ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics'): set configuration segment.ms to 14400000 (org.apache.kafka.controller.ConfigurationControlManager)
[2022-10-10 14:47:48,200] INFO [Controller 2] ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
[2022-10-10 14:47:48,200] INFO [Controller 2] Created topic _confluent-telemetry-metrics with topic ID UDnB-OUDRe2M16eIPtTU6Q. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:47:48,200] INFO [Controller 2] Created partition _confluent-telemetry-metrics-0 with topic ID UDnB-OUDRe2M16eIPtTU6Q and PartitionRegistration(replicas=[2, 3, 1], isr=[2, 3, 1], removingReplicas=[], addingReplicas=[], leader=2, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:47:48,200] INFO [Controller 2] Created partition _confluent-telemetry-metrics-1 with topic ID UDnB-OUDRe2M16eIPtTU6Q and PartitionRegistration(replicas=[2, 1, 3], isr=[2, 1, 3], removingReplicas=[], addingReplicas=[], leader=2, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:47:48,200] INFO [Controller 2] Created partition _confluent-telemetry-metrics-2 with topic ID UDnB-OUDRe2M16eIPtTU6Q and PartitionRegistration(replicas=[2, 3, 1], isr=[2, 3, 1], removingReplicas=[], addingReplicas=[], leader=2, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:47:48,200] INFO [Controller 2] Created partition _confluent-telemetry-metrics-3 with topic ID UDnB-OUDRe2M16eIPtTU6Q and PartitionRegistration(replicas=[2, 1, 3], isr=[2, 1, 3], removingReplicas=[], addingReplicas=[], leader=2, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:47:48,201] INFO [Controller 2] Created partition _confluent-telemetry-metrics-4 with topic ID UDnB-OUDRe2M16eIPtTU6Q and PartitionRegistration(replicas=[2, 3, 1], isr=[2, 3, 1], removingReplicas=[], addingReplicas=[], leader=2, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:47:48,201] INFO [Controller 2] Created partition _confluent-telemetry-metrics-5 with topic ID UDnB-OUDRe2M16eIPtTU6Q and PartitionRegistration(replicas=[2, 1, 3], isr=[2, 1, 3], removingReplicas=[], addingReplicas=[], leader=2, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:47:48,201] INFO [Controller 2] Created partition _confluent-telemetry-metrics-6 with topic ID UDnB-OUDRe2M16eIPtTU6Q and PartitionRegistration(replicas=[2, 3, 1], isr=[2, 3, 1], removingReplicas=[], addingReplicas=[], leader=2, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:47:48,201] INFO [Controller 2] Created partition _confluent-telemetry-metrics-7 with topic ID UDnB-OUDRe2M16eIPtTU6Q and PartitionRegistration(replicas=[2, 1, 3], isr=[2, 1, 3], removingReplicas=[], addingReplicas=[], leader=2, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:47:48,201] INFO [Controller 2] Created partition _confluent-telemetry-metrics-8 with topic ID UDnB-OUDRe2M16eIPtTU6Q and PartitionRegistration(replicas=[2, 3, 1], isr=[2, 3, 1], removingReplicas=[], addingReplicas=[], leader=2, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:47:48,201] INFO [Controller 2] Created partition _confluent-telemetry-metrics-9 with topic ID UDnB-OUDRe2M16eIPtTU6Q and PartitionRegistration(replicas=[2, 1, 3], isr=[2, 1, 3], removingReplicas=[], addingReplicas=[], leader=2, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:47:48,201] INFO [Controller 2] Created partition _confluent-telemetry-metrics-10 with topic ID UDnB-OUDRe2M16eIPtTU6Q and PartitionRegistration(replicas=[2, 3, 1], isr=[2, 3, 1], removingReplicas=[], addingReplicas=[], leader=2, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:47:48,201] INFO [Controller 2] Created partition _confluent-telemetry-metrics-11 with topic ID UDnB-OUDRe2M16eIPtTU6Q and PartitionRegistration(replicas=[2, 1, 3], isr=[2, 1, 3], removingReplicas=[], addingReplicas=[], leader=2, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:47:58,037] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:47:58,039] INFO [Controller 2] handleBrokerFenced: changing 13 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:47:58,041] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:48:18,200] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:48:18,201] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 0 due to node 2 being disconnected (elapsed time since creation: 30002ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:48:18,201] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:48:18,254] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:48:18,265] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:48:19,034] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:48:19,034] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 34 due to node 2 being disconnected (elapsed time since creation: 30001ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:48:19,034] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:48:19,035] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 14:48:19,153] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 14:48:19,155] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:48:19,155] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:48:28,156] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:48:28,157] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:48:28,159] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:48:48,190] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:48:48,191] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 60001ms, elapsed time since send: 60001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:48:48,194] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:48:48,196] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 14:48:48,203] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 14:48:48,207] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:48:48,207] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:48:48,207] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:48:48,208] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Call(callName=createTopics, deadlineMs=1665406128189, tries=1, nextAllowedTryMs=1665406128692) timed out at 1665406128192 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled createTopics request with correlation id 4 due to node 2 being disconnected
[2022-10-10 14:48:48,219] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:48:48,223] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:48:48,223] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:48:48,223] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:48:48,223] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:48:48,223] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:48:48,223] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:48:48,223] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:48:48,223] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:48:48,223] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:48:48,224] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:48:48,224] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:48:48,224] INFO Kafka startTimeMs: 1665406128224 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:48:49,151] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:48:49,151] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 35 due to node 2 being disconnected (elapsed time since creation: 30009ms, elapsed time since send: 30006ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:48:49,151] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:48:49,152] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 14:48:49,358] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 14:48:49,358] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:48:49,359] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:48:58,361] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:48:58,362] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:48:58,362] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:49:18,232] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:49:18,233] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 3 due to node 2 being disconnected (elapsed time since creation: 59977ms, elapsed time since send: 59967ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:49:18,233] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:49:18,283] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:49:18,301] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:49:19,358] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:49:19,359] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 37 due to node 2 being disconnected (elapsed time since creation: 30005ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:49:19,359] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:49:19,359] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 14:49:19,779] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 14:49:19,779] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:49:19,780] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:49:28,784] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:49:28,785] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:49:28,787] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:49:48,232] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:49:48,233] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 60002ms, elapsed time since send: 60002ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:49:48,234] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:49:48,234] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 14:49:48,234] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 14:49:48,235] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:49:48,235] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:49:48,236] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:49:48,236] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Call(callName=createTopics, deadlineMs=1665406188229, tries=1, nextAllowedTryMs=1665406188733) timed out at 1665406188233 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled createTopics request with correlation id 4 due to node 2 being disconnected
[2022-10-10 14:49:48,263] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:49:48,264] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:49:48,264] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:49:48,264] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:49:48,264] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:49:48,264] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:49:48,264] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:49:48,264] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:49:48,264] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:49:48,264] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:49:48,264] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:49:48,264] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:49:48,264] INFO Kafka startTimeMs: 1665406188264 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:49:49,780] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:49:49,780] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 39 due to node 2 being disconnected (elapsed time since creation: 30011ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:49:49,781] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:49:49,781] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 14:49:50,601] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 14:49:50,601] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:49:50,602] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:49:59,605] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:49:59,606] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:49:59,606] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:50:18,270] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:50:18,272] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 7 due to node 2 being disconnected (elapsed time since creation: 59987ms, elapsed time since send: 59969ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:50:18,273] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:50:18,322] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:50:18,328] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:50:20,600] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:50:20,600] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 41 due to node 2 being disconnected (elapsed time since creation: 30013ms, elapsed time since send: 30007ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:50:20,600] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:50:20,600] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 14:50:22,214] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 14:50:22,216] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:50:22,216] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:50:31,216] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:50:31,217] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:50:31,218] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:50:48,270] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:50:48,271] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 60002ms, elapsed time since send: 60002ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:50:48,273] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:50:48,273] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 14:50:48,273] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 14:50:48,275] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:50:48,275] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:50:48,275] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:50:48,275] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Call(callName=createTopics, deadlineMs=1665406248268, tries=1, nextAllowedTryMs=1665406248772) timed out at 1665406248272 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled createTopics request with correlation id 4 due to node 2 being disconnected
[2022-10-10 14:50:48,280] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:50:48,284] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:50:48,285] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:50:48,285] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:50:48,285] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:50:48,285] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:50:48,285] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:50:48,285] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:50:48,285] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:50:48,285] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:50:48,285] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:50:48,285] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:50:48,285] INFO Kafka startTimeMs: 1665406248285 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:50:52,215] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:50:52,216] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 43 due to node 2 being disconnected (elapsed time since creation: 30009ms, elapsed time since send: 30002ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:50:52,216] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:50:52,216] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 14:50:55,434] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 14:50:55,435] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:50:55,435] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:51:04,441] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:51:04,444] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:51:04,445] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:51:18,305] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:51:18,310] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 11 due to node 2 being disconnected (elapsed time since creation: 59981ms, elapsed time since send: 59978ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:51:18,310] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:51:18,356] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:51:18,364] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:51:25,435] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:51:25,436] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 45 due to node 2 being disconnected (elapsed time since creation: 30009ms, elapsed time since send: 30002ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:51:25,437] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:51:25,437] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 14:51:31,875] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 14:51:31,877] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:51:31,877] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:51:40,881] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:51:40,882] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:51:40,885] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:51:48,306] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:51:48,306] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 60002ms, elapsed time since send: 60002ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:51:48,308] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:51:48,308] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 14:51:48,308] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 14:51:48,314] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:51:48,314] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:51:48,314] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:51:48,316] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Call(callName=createTopics, deadlineMs=1665406308303, tries=1, nextAllowedTryMs=1665406308806) timed out at 1665406308306 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled createTopics request with correlation id 4 due to node 2 being disconnected
[2022-10-10 14:51:48,318] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:51:48,320] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:51:48,320] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:51:48,320] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:51:48,320] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:51:48,320] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:51:48,320] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:51:48,320] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:51:48,320] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:51:48,320] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:51:48,320] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:51:48,320] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:51:48,321] INFO Kafka startTimeMs: 1665406308320 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:52:01,877] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:52:01,878] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 47 due to node 2 being disconnected (elapsed time since creation: 30010ms, elapsed time since send: 30002ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:52:01,878] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:52:01,878] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 14:52:11,037] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 14:52:11,038] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:52:11,038] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:52:18,335] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:52:18,335] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 15 due to node 2 being disconnected (elapsed time since creation: 59978ms, elapsed time since send: 59974ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:52:18,335] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:52:18,385] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:52:18,394] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:52:20,038] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:52:20,043] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:52:20,045] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:52:48,325] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:52:48,326] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 59994ms, elapsed time since send: 59994ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:52:48,334] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:52:48,334] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 14:52:48,334] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 14:52:48,336] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:52:48,337] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:52:48,337] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:52:48,337] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: createTopics
[2022-10-10 14:52:48,339] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:52:48,347] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:52:48,347] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:52:48,347] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:52:48,347] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:52:48,347] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:52:48,348] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:52:48,348] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:52:48,348] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:52:48,348] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:52:48,348] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:52:48,348] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:52:48,348] INFO Kafka startTimeMs: 1665406368348 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:53:11,035] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:53:11,036] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 49 due to node 2 being disconnected (elapsed time since creation: 60000ms, elapsed time since send: 59997ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:53:11,036] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:53:11,036] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 14:53:18,361] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:53:18,362] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 19 due to node 2 being disconnected (elapsed time since creation: 59974ms, elapsed time since send: 59967ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:53:18,363] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:53:18,411] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:53:18,422] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:53:19,985] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 14:53:19,986] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:53:19,986] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:53:28,989] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:53:28,992] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:53:28,993] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:53:48,360] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:53:48,361] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 60000ms, elapsed time since send: 60000ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:53:48,361] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:53:48,361] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 14:53:48,361] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 14:53:48,364] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:53:48,365] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:53:48,366] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:53:48,369] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Call(callName=createTopics, deadlineMs=1665406428360, tries=1, nextAllowedTryMs=1665406428861) timed out at 1665406428361 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled createTopics request with correlation id 4 due to node 2 being disconnected
[2022-10-10 14:53:48,371] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:53:48,373] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:53:48,373] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:53:48,373] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:53:48,373] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:53:48,373] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:53:48,373] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:53:48,373] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:53:48,373] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:53:48,373] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:53:48,373] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:53:48,374] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:53:48,374] INFO Kafka startTimeMs: 1665406428373 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:53:49,985] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:53:49,986] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 51 due to node 2 being disconnected (elapsed time since creation: 30002ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:53:49,986] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:53:49,986] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 14:53:58,948] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 14:53:58,955] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:53:58,956] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:54:07,960] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:54:07,961] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:54:07,961] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:54:18,393] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:54:18,395] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 23 due to node 2 being disconnected (elapsed time since creation: 59982ms, elapsed time since send: 59975ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:54:18,395] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:54:18,445] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:54:18,460] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:54:28,943] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:54:28,943] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 53 due to node 2 being disconnected (elapsed time since creation: 30004ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:54:28,944] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:54:28,944] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 14:54:38,073] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 14:54:38,075] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:54:38,076] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:54:47,083] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:54:47,084] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:54:47,084] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:54:48,393] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:54:48,393] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 60001ms, elapsed time since send: 60001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:54:48,394] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:54:48,395] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 14:54:48,395] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 14:54:48,397] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:54:48,397] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:54:48,397] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:54:48,398] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Call(callName=createTopics, deadlineMs=1665406488391, tries=1, nextAllowedTryMs=1665406488893) timed out at 1665406488393 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled createTopics request with correlation id 4 due to node 2 being disconnected
[2022-10-10 14:54:48,399] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:54:48,401] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:54:48,402] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:54:48,403] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:54:48,403] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:54:48,403] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:54:48,403] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:54:48,403] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:54:48,403] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:54:48,404] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:54:48,404] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:54:48,404] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:54:48,404] INFO Kafka startTimeMs: 1665406488404 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:55:08,076] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:55:08,078] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 55 due to node 2 being disconnected (elapsed time since creation: 30007ms, elapsed time since send: 30002ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:55:08,078] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:55:08,078] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 14:55:17,085] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 14:55:17,085] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:55:17,086] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:55:18,415] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:55:18,415] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 27 due to node 2 being disconnected (elapsed time since creation: 59969ms, elapsed time since send: 59961ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:55:18,416] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:55:18,465] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:55:18,477] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:55:26,089] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:55:26,090] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:55:26,090] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:55:47,085] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:55:47,086] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 57 due to node 2 being disconnected (elapsed time since creation: 30006ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:55:47,087] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:55:47,087] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 14:55:48,413] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:55:48,414] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 60000ms, elapsed time since send: 60000ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:55:48,418] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:55:48,418] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 14:55:48,418] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 14:55:48,419] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:55:48,419] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:55:48,419] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:55:48,420] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Call(callName=createTopics, deadlineMs=1665406548412, tries=1, nextAllowedTryMs=1665406548914) timed out at 1665406548414 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled createTopics request with correlation id 4 due to node 2 being disconnected
[2022-10-10 14:55:48,432] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:55:48,434] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:55:48,434] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:55:48,434] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:55:48,434] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:55:48,434] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:55:48,434] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:55:48,434] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:55:48,434] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:55:48,434] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:55:48,434] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:55:48,434] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:55:48,434] INFO Kafka startTimeMs: 1665406548434 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:55:49,262] INFO [Producer clientId=confluent-telemetry-reporter-local-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:55:50,135] INFO [Producer clientId=_confluent-license-producer-2] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:55:50,428] INFO [Consumer clientId=_confluent-license-consumer-2, groupId=null] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:55:56,104] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 14:55:56,106] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:55:56,106] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:56:05,109] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:56:05,110] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:56:05,111] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:56:18,440] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:56:18,440] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 31 due to node 2 being disconnected (elapsed time since creation: 59974ms, elapsed time since send: 59965ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:56:18,441] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:56:18,490] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:56:18,497] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:56:26,107] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:56:26,107] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 59 due to node 2 being disconnected (elapsed time since creation: 30005ms, elapsed time since send: 30002ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:56:26,107] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:56:26,107] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 14:56:35,266] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 14:56:35,267] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:56:35,267] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:56:44,272] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:56:44,274] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:56:44,276] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:56:48,439] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:56:48,440] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 60001ms, elapsed time since send: 60001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:56:48,442] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:56:48,442] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 14:56:48,442] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 14:56:48,444] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:56:48,444] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:56:48,444] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:56:48,448] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Call(callName=createTopics, deadlineMs=1665406608438, tries=1, nextAllowedTryMs=1665406608941) timed out at 1665406608441 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled createTopics request with correlation id 4 due to node 2 being disconnected
[2022-10-10 14:56:48,451] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:56:48,456] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:56:48,457] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:56:48,457] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:56:48,457] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:56:48,457] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:56:48,457] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:56:48,457] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:56:48,457] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:56:48,457] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:56:48,457] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:56:48,459] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:56:48,459] INFO Kafka startTimeMs: 1665406608457 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:57:05,267] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:57:05,268] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 61 due to node 2 being disconnected (elapsed time since creation: 30015ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:57:05,269] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:57:05,269] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 14:57:14,124] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 14:57:14,127] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:57:14,127] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:57:18,469] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:57:18,469] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 35 due to node 2 being disconnected (elapsed time since creation: 59979ms, elapsed time since send: 59974ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:57:18,469] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:57:18,521] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:57:18,533] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:57:23,131] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:57:23,132] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:57:23,132] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:57:44,126] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:57:44,126] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 63 due to node 2 being disconnected (elapsed time since creation: 30005ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:57:44,127] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:57:44,127] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 14:57:48,468] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:57:48,469] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 60001ms, elapsed time since send: 60001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:57:48,469] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:57:48,469] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 14:57:48,469] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 14:57:48,470] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:57:48,471] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:57:48,471] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:57:48,471] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Call(callName=createTopics, deadlineMs=1665406668467, tries=1, nextAllowedTryMs=1665406668969) timed out at 1665406668469 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled createTopics request with correlation id 4 due to node 2 being disconnected
[2022-10-10 14:57:48,472] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:57:48,479] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:57:48,480] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:57:48,480] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:57:48,480] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:57:48,480] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:57:48,480] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:57:48,480] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:57:48,480] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:57:48,480] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:57:48,480] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:57:48,480] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:57:48,480] INFO Kafka startTimeMs: 1665406668480 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:57:53,143] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 14:57:53,144] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:57:53,144] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:58:02,148] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:58:02,148] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:58:02,148] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:58:18,494] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:58:18,496] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 39 due to node 2 being disconnected (elapsed time since creation: 59973ms, elapsed time since send: 59962ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:58:18,496] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:58:18,545] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:58:18,556] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:58:23,144] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:58:23,145] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 65 due to node 2 being disconnected (elapsed time since creation: 30006ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:58:23,145] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:58:23,145] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 14:58:32,293] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 14:58:32,294] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:58:32,294] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:58:41,298] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:58:41,298] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:58:41,299] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:58:48,494] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:58:48,500] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 60001ms, elapsed time since send: 60001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:58:48,501] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:58:48,501] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 14:58:48,501] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 14:58:48,502] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:58:48,502] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:58:48,502] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:58:48,502] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Call(callName=createTopics, deadlineMs=1665406728492, tries=1, nextAllowedTryMs=1665406729000) timed out at 1665406728500 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled createTopics request with correlation id 4 due to node 2 being disconnected
[2022-10-10 14:58:48,504] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:58:48,506] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:58:48,506] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:58:48,506] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:58:48,506] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:58:48,506] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:58:48,506] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:58:48,506] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:58:48,506] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:58:48,506] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:58:48,506] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:58:48,507] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:58:48,507] INFO Kafka startTimeMs: 1665406728506 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:59:02,294] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:59:02,294] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 67 due to node 2 being disconnected (elapsed time since creation: 30003ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:59:02,294] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:59:02,294] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 14:59:11,415] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 14:59:11,416] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:59:11,417] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:59:18,516] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:59:18,517] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 43 due to node 2 being disconnected (elapsed time since creation: 59970ms, elapsed time since send: 59961ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:59:18,518] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:59:18,567] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:59:18,576] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:59:20,421] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:59:20,422] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:59:20,423] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:59:41,417] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:59:41,417] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 69 due to node 2 being disconnected (elapsed time since creation: 30006ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:59:41,417] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 14:59:41,417] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 14:59:48,516] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:59:48,517] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 60001ms, elapsed time since send: 60001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 14:59:48,518] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:59:48,519] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 14:59:48,519] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 14:59:48,520] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:59:48,520] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:59:48,520] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 14:59:48,521] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Call(callName=createTopics, deadlineMs=1665406788513, tries=1, nextAllowedTryMs=1665406789017) timed out at 1665406788517 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled createTopics request with correlation id 4 due to node 2 being disconnected
[2022-10-10 14:59:48,522] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:59:48,529] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:59:48,529] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:59:48,529] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:59:48,529] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:59:48,529] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:59:48,529] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:59:48,529] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:59:48,529] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:59:48,529] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 14:59:48,529] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:59:48,529] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:59:48,529] INFO Kafka startTimeMs: 1665406788529 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 14:59:50,503] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 14:59:50,504] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:59:50,505] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 14:59:59,506] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:59:59,507] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 14:59:59,508] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:00:18,553] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:00:18,559] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 47 due to node 2 being disconnected (elapsed time since creation: 59984ms, elapsed time since send: 59977ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:00:18,559] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:00:18,602] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:00:18,612] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:00:20,505] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:00:20,506] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 71 due to node 2 being disconnected (elapsed time since creation: 30007ms, elapsed time since send: 30002ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:00:20,506] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:00:20,506] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 15:00:29,467] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 15:00:29,469] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:00:29,469] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:00:38,474] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:00:38,475] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:00:38,476] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:00:48,553] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:00:48,554] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 60002ms, elapsed time since send: 60002ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:00:48,557] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:00:48,558] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 15:00:48,558] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 15:00:48,560] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:00:48,560] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:00:48,560] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:00:48,561] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Call(callName=createTopics, deadlineMs=1665406848550, tries=1, nextAllowedTryMs=1665406849055) timed out at 1665406848555 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled createTopics request with correlation id 4 due to node 2 being disconnected
[2022-10-10 15:00:48,564] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:00:48,567] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:00:48,567] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:00:48,567] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:00:48,568] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:00:48,568] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:00:48,568] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:00:48,568] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:00:48,568] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:00:48,568] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:00:48,568] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:00:48,568] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:00:48,568] INFO Kafka startTimeMs: 1665406848568 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:00:59,467] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:00:59,468] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 73 due to node 2 being disconnected (elapsed time since creation: 30005ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:00:59,468] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:00:59,468] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 15:01:08,373] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 15:01:08,374] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:01:08,374] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:01:17,378] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:01:17,382] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:01:17,383] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:01:18,580] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:01:18,581] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 51 due to node 2 being disconnected (elapsed time since creation: 59977ms, elapsed time since send: 59969ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:01:18,581] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:01:18,632] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:01:18,644] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:01:38,375] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:01:38,375] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 75 due to node 2 being disconnected (elapsed time since creation: 30006ms, elapsed time since send: 30002ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:01:38,375] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:01:38,375] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 15:01:47,282] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 15:01:47,284] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:01:47,285] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:01:48,581] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:01:48,582] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 60002ms, elapsed time since send: 60002ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:01:48,583] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:01:48,583] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 15:01:48,583] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 15:01:48,584] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:01:48,585] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:01:48,585] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:01:48,585] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Call(callName=createTopics, deadlineMs=1665406908578, tries=1, nextAllowedTryMs=1665406909082) timed out at 1665406908582 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled createTopics request with correlation id 4 due to node 2 being disconnected
[2022-10-10 15:01:48,589] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:01:48,596] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:01:48,596] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:01:48,596] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:01:48,596] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:01:48,596] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:01:48,596] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:01:48,596] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:01:48,596] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:01:48,596] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:01:48,596] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:01:48,596] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:01:48,597] INFO Kafka startTimeMs: 1665406908596 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:01:56,287] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:01:56,287] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:01:56,288] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:02:17,283] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:02:17,284] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 77 due to node 2 being disconnected (elapsed time since creation: 30007ms, elapsed time since send: 30002ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:02:17,284] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:02:17,284] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 15:02:18,606] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:02:18,606] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 55 due to node 2 being disconnected (elapsed time since creation: 59972ms, elapsed time since send: 59965ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:02:18,606] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:02:18,657] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:02:18,664] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:02:26,424] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 15:02:26,425] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:02:26,425] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:02:35,427] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:02:35,427] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:02:35,427] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:02:48,606] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:02:48,607] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 60001ms, elapsed time since send: 60001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:02:48,609] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:02:48,609] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 15:02:48,609] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 15:02:48,610] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:02:48,610] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:02:48,610] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:02:48,611] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Call(callName=createTopics, deadlineMs=1665406968604, tries=1, nextAllowedTryMs=1665406969107) timed out at 1665406968607 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled createTopics request with correlation id 4 due to node 2 being disconnected
[2022-10-10 15:02:48,612] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:02:48,615] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:02:48,619] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:02:48,619] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:02:48,619] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:02:48,619] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:02:48,619] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:02:48,619] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:02:48,619] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:02:48,620] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:02:48,620] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:02:48,620] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:02:48,620] INFO Kafka startTimeMs: 1665406968620 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:03:18,636] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:03:18,639] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 59 due to node 2 being disconnected (elapsed time since creation: 59979ms, elapsed time since send: 59973ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:03:18,639] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:03:18,686] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:03:18,692] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:03:26,426] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:03:26,426] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 79 due to node 2 being disconnected (elapsed time since creation: 60004ms, elapsed time since send: 60002ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:03:26,426] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:03:26,427] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 15:03:35,575] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 15:03:35,576] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:03:35,577] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:03:44,580] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:03:44,581] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:03:44,581] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:03:48,628] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:03:48,628] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 60001ms, elapsed time since send: 60001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:03:48,629] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:03:48,629] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 15:03:48,629] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 15:03:48,631] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:03:48,631] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:03:48,631] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:03:48,631] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Call(callName=createTopics, deadlineMs=1665407028626, tries=1, nextAllowedTryMs=1665407029128) timed out at 1665407028628 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled createTopics request with correlation id 4 due to node 2 being disconnected
[2022-10-10 15:03:48,633] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:03:48,638] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:03:48,638] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:03:48,638] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:03:48,638] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:03:48,638] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:03:48,638] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:03:48,638] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:03:48,638] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:03:48,638] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:03:48,638] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:03:48,639] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:03:48,639] INFO Kafka startTimeMs: 1665407028638 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:04:05,575] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:04:05,576] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 81 due to node 2 being disconnected (elapsed time since creation: 30004ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:04:05,576] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:04:05,576] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 15:04:14,473] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 15:04:14,474] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:04:14,475] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:04:18,657] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:04:18,658] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 63 due to node 2 being disconnected (elapsed time since creation: 59970ms, elapsed time since send: 59965ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:04:18,658] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:04:18,707] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:04:18,712] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:04:23,479] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:04:23,481] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:04:23,481] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:04:44,474] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:04:44,478] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 83 due to node 2 being disconnected (elapsed time since creation: 30011ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:04:44,478] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:04:44,478] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 15:04:48,654] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:04:48,655] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 60001ms, elapsed time since send: 60001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:04:48,656] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:04:48,656] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 15:04:48,657] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 15:04:48,662] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:04:48,663] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:04:48,663] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:04:48,664] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Call(callName=createTopics, deadlineMs=1665407088653, tries=1, nextAllowedTryMs=1665407089155) timed out at 1665407088655 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled createTopics request with correlation id 4 due to node 2 being disconnected
[2022-10-10 15:04:48,669] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:04:48,673] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:04:48,673] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:04:48,673] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:04:48,673] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:04:48,673] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:04:48,673] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:04:48,673] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:04:48,673] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:04:48,673] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:04:48,673] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:04:48,673] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:04:48,673] INFO Kafka startTimeMs: 1665407088673 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:04:53,477] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 15:04:53,478] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:04:53,484] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:05:02,483] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:05:02,485] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:05:02,486] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:05:18,693] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:05:18,694] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 67 due to node 2 being disconnected (elapsed time since creation: 59985ms, elapsed time since send: 59983ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:05:18,694] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:05:18,745] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:05:18,753] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:05:23,479] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:05:23,479] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 85 due to node 2 being disconnected (elapsed time since creation: 30007ms, elapsed time since send: 30002ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:05:23,480] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:05:23,480] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 15:05:32,543] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 15:05:32,544] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:05:32,545] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:05:41,549] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:05:41,550] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:05:41,550] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:05:48,692] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:05:48,693] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 60001ms, elapsed time since send: 60001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:05:48,694] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:05:48,694] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 15:05:48,694] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 15:05:48,695] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:05:48,695] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:05:48,695] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:05:48,695] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Call(callName=createTopics, deadlineMs=1665407148691, tries=1, nextAllowedTryMs=1665407149193) timed out at 1665407148693 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled createTopics request with correlation id 4 due to node 2 being disconnected
[2022-10-10 15:05:48,698] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:05:48,700] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:05:48,700] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:05:48,700] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:05:48,700] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:05:48,700] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:05:48,700] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:05:48,700] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:05:48,700] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:05:48,701] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:05:48,701] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:05:48,701] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:05:48,701] INFO Kafka startTimeMs: 1665407148701 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:06:02,543] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:06:02,544] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 87 due to node 2 being disconnected (elapsed time since creation: 30005ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:06:02,544] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:06:02,544] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 15:06:11,570] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 15:06:11,571] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:06:11,571] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:06:18,715] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:06:18,716] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 71 due to node 2 being disconnected (elapsed time since creation: 59970ms, elapsed time since send: 59963ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:06:18,716] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:06:18,766] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:06:18,775] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:06:20,576] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:06:20,577] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:06:20,577] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:06:41,572] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:06:41,573] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 89 due to node 2 being disconnected (elapsed time since creation: 30006ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:06:41,574] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:06:41,574] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 15:06:48,714] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:06:48,717] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 60001ms, elapsed time since send: 60001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:06:48,718] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:06:48,718] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 15:06:48,718] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 15:06:48,719] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:06:48,720] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:06:48,720] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:06:48,720] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Call(callName=createTopics, deadlineMs=1665407208712, tries=1, nextAllowedTryMs=1665407209217) timed out at 1665407208717 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled createTopics request with correlation id 4 due to node 2 being disconnected
[2022-10-10 15:06:48,721] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:06:48,733] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:06:48,733] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:06:48,733] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:06:48,733] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:06:48,733] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:06:48,733] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:06:48,733] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:06:48,733] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:06:48,734] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:06:48,734] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:06:48,734] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:06:48,734] INFO Kafka startTimeMs: 1665407208734 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:06:50,509] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 15:06:50,510] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:06:50,511] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:06:59,514] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:06:59,515] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:06:59,515] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:07:18,742] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:07:18,742] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 75 due to node 2 being disconnected (elapsed time since creation: 59976ms, elapsed time since send: 59968ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:07:18,743] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:07:18,792] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:07:18,796] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:07:20,510] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:07:20,511] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 91 due to node 2 being disconnected (elapsed time since creation: 30007ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:07:20,511] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:07:20,511] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 15:07:29,367] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 15:07:29,368] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:07:29,368] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:07:38,358] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:07:38,359] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:07:38,359] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:07:48,719] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:07:48,719] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 59977ms, elapsed time since send: 59977ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:07:48,743] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:07:48,743] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 15:07:48,743] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 15:07:48,744] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:07:48,744] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:07:48,744] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:07:48,745] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: createTopics
[2022-10-10 15:07:48,746] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:07:48,747] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:07:48,747] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:07:48,747] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:07:48,747] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:07:48,747] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:07:48,747] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:07:48,747] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:07:48,747] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:07:48,748] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:07:48,748] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:07:48,748] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:07:48,748] INFO Kafka startTimeMs: 1665407268748 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:08:18,748] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:08:18,750] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 79 due to node 2 being disconnected (elapsed time since creation: 59955ms, elapsed time since send: 59953ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:08:18,750] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:08:18,799] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:08:18,810] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:08:29,328] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:08:29,329] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 93 due to node 2 being disconnected (elapsed time since creation: 59983ms, elapsed time since send: 59969ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:08:29,329] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:08:29,329] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 15:08:38,248] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 15:08:38,250] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:08:38,252] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:08:47,255] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:08:47,256] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:08:47,257] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:08:48,747] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:08:48,747] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 59988ms, elapsed time since send: 59988ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:08:48,760] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:08:48,762] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 15:08:48,762] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 15:08:48,764] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:08:48,764] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:08:48,764] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:08:48,765] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: createTopics
[2022-10-10 15:08:48,767] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:08:48,771] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:08:48,771] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:08:48,771] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:08:48,771] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:08:48,771] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:08:48,771] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:08:48,771] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:08:48,771] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:08:48,771] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:08:48,771] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:08:48,771] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:08:48,771] INFO Kafka startTimeMs: 1665407328771 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:09:08,248] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:09:08,249] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 95 due to node 2 being disconnected (elapsed time since creation: 30006ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:09:08,249] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:09:08,249] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 15:09:17,160] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 15:09:17,160] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:09:17,160] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:09:18,781] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:09:18,781] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 83 due to node 2 being disconnected (elapsed time since creation: 59982ms, elapsed time since send: 59972ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:09:18,781] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:09:18,831] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:09:18,839] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:09:26,165] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:09:26,166] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:09:26,167] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:09:47,161] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:09:47,162] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 97 due to node 2 being disconnected (elapsed time since creation: 30004ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:09:47,162] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:09:47,162] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 15:09:48,780] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:09:48,780] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 60000ms, elapsed time since send: 60000ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:09:48,781] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:09:48,782] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 15:09:48,782] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 15:09:48,785] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:09:48,785] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:09:48,785] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:09:48,785] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Call(callName=createTopics, deadlineMs=1665407388780, tries=1, nextAllowedTryMs=1665407389280) timed out at 1665407388780 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled createTopics request with correlation id 4 due to node 2 being disconnected
[2022-10-10 15:09:48,787] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:09:48,797] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:09:48,798] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:09:48,798] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:09:48,798] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:09:48,798] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:09:48,798] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:09:48,798] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:09:48,798] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:09:48,798] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:09:48,798] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:09:48,798] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:09:48,798] INFO Kafka startTimeMs: 1665407388798 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:09:56,273] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 15:09:56,273] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:09:56,273] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:10:05,276] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:10:05,276] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:10:05,277] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:10:18,805] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:10:18,805] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 87 due to node 2 being disconnected (elapsed time since creation: 59973ms, elapsed time since send: 59966ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:10:18,805] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:10:18,856] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:10:18,865] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:10:26,274] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:10:26,275] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 99 due to node 2 being disconnected (elapsed time since creation: 30002ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:10:26,275] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:10:26,275] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 15:10:35,394] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 15:10:35,395] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:10:35,395] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:10:44,399] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:10:44,400] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:10:44,400] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:10:48,805] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:10:48,806] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 60001ms, elapsed time since send: 60001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:10:48,808] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:10:48,808] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 15:10:48,809] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 15:10:48,811] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:10:48,811] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:10:48,811] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:10:48,813] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Call(callName=createTopics, deadlineMs=1665407448803, tries=1, nextAllowedTryMs=1665407449306) timed out at 1665407448806 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled createTopics request with correlation id 4 due to node 2 being disconnected
[2022-10-10 15:10:48,814] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:10:48,818] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:10:48,818] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:10:48,818] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:10:48,818] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:10:48,818] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:10:48,818] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:10:48,820] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:10:48,820] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:10:48,820] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:10:48,821] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:10:48,821] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:10:48,821] INFO Kafka startTimeMs: 1665407448820 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:11:05,395] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:11:05,396] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 101 due to node 2 being disconnected (elapsed time since creation: 30004ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:11:05,396] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:11:05,396] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 15:11:14,542] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 15:11:14,543] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:11:14,544] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:11:18,831] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:11:18,832] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 91 due to node 2 being disconnected (elapsed time since creation: 59974ms, elapsed time since send: 59966ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:11:18,832] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:11:18,882] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:11:18,888] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:11:23,546] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:11:23,547] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:11:23,547] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:11:44,544] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:11:44,544] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 103 due to node 2 being disconnected (elapsed time since creation: 30009ms, elapsed time since send: 30003ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:11:44,544] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:11:44,544] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 15:11:48,831] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:11:48,831] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 60000ms, elapsed time since send: 60000ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:11:48,832] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:11:48,832] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 15:11:48,832] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 15:11:48,832] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:11:48,832] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:11:48,832] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:11:48,832] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Call(callName=createTopics, deadlineMs=1665407508830, tries=1, nextAllowedTryMs=1665407509331) timed out at 1665407508831 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled createTopics request with correlation id 4 due to node 2 being disconnected
[2022-10-10 15:11:48,833] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:11:48,835] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:11:48,835] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:11:48,835] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:11:48,835] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:11:48,835] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:11:48,835] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:11:48,835] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:11:48,835] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:11:48,835] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:11:48,835] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:11:48,835] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:11:48,835] INFO Kafka startTimeMs: 1665407508835 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:11:53,411] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 15:11:53,411] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:11:53,412] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:12:02,417] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:12:02,417] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:12:02,417] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:12:18,841] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:12:18,842] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 95 due to node 2 being disconnected (elapsed time since creation: 59958ms, elapsed time since send: 59953ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:12:18,842] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:12:18,891] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:12:18,899] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:12:23,412] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:12:23,413] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 105 due to node 2 being disconnected (elapsed time since creation: 30003ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:12:23,413] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:12:23,413] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 15:12:32,342] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 15:12:32,344] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:12:32,344] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:12:41,344] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:12:41,344] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:12:41,345] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:12:48,841] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:12:48,843] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 60000ms, elapsed time since send: 60000ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:12:48,844] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:12:48,844] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 15:12:48,844] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 15:12:48,846] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:12:48,846] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:12:48,846] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:12:48,847] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Call(callName=createTopics, deadlineMs=1665407568839, tries=1, nextAllowedTryMs=1665407569343) timed out at 1665407568843 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled createTopics request with correlation id 4 due to node 2 being disconnected
[2022-10-10 15:12:48,849] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:12:48,853] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:12:48,853] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:12:48,853] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:12:48,853] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:12:48,853] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:12:48,853] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:12:48,853] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:12:48,853] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:12:48,853] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:12:48,853] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:12:48,853] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:12:48,853] INFO Kafka startTimeMs: 1665407568853 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:13:18,869] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:13:18,871] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 99 due to node 2 being disconnected (elapsed time since creation: 59977ms, elapsed time since send: 59971ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:13:18,871] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:13:18,920] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:13:18,923] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:13:32,343] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:13:32,343] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 107 due to node 2 being disconnected (elapsed time since creation: 60003ms, elapsed time since send: 60000ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:13:32,344] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:13:32,344] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 15:13:41,363] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 15:13:41,364] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:13:41,364] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:13:48,869] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:13:48,870] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 60001ms, elapsed time since send: 60001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:13:48,871] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:13:48,871] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 15:13:48,871] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 15:13:48,873] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:13:48,873] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:13:48,873] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:13:48,873] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Call(callName=createTopics, deadlineMs=1665407628868, tries=1, nextAllowedTryMs=1665407629370) timed out at 1665407628870 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled createTopics request with correlation id 4 due to node 2 being disconnected
[2022-10-10 15:13:48,878] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:13:48,883] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:13:48,883] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:13:48,884] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:13:48,884] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:13:48,884] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:13:48,884] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:13:48,889] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:13:48,889] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:13:48,889] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:13:48,889] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:13:48,889] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:13:48,889] INFO Kafka startTimeMs: 1665407628889 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:13:50,369] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:13:50,369] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:13:50,369] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:14:11,364] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:14:11,366] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 109 due to node 2 being disconnected (elapsed time since creation: 30002ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:14:11,366] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:14:11,366] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 15:14:18,898] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:14:18,899] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 103 due to node 2 being disconnected (elapsed time since creation: 59977ms, elapsed time since send: 59975ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:14:18,899] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:14:18,949] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:14:18,954] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:14:20,200] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 15:14:20,200] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:14:20,200] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:14:29,202] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:14:29,205] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:14:29,206] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:14:48,898] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:14:48,899] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 60001ms, elapsed time since send: 60001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:14:48,900] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:14:48,901] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 15:14:48,901] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 15:14:48,903] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:14:48,903] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:14:48,903] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:14:48,903] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Call(callName=createTopics, deadlineMs=1665407688897, tries=1, nextAllowedTryMs=1665407689399) timed out at 1665407688899 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled createTopics request with correlation id 4 due to node 2 being disconnected
[2022-10-10 15:14:48,904] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:14:48,906] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:14:48,906] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:14:48,906] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:14:48,906] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:14:48,906] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:14:48,906] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:14:48,906] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:14:48,906] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:14:48,907] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:14:48,907] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:14:48,907] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:14:48,907] INFO Kafka startTimeMs: 1665407688907 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:14:50,200] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:14:50,201] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 111 due to node 2 being disconnected (elapsed time since creation: 30003ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:14:50,201] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:14:50,201] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 15:14:59,158] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 15:14:59,158] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:14:59,159] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:15:08,161] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:15:08,161] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:15:08,162] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:15:18,915] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:15:18,917] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 107 due to node 2 being disconnected (elapsed time since creation: 59965ms, elapsed time since send: 59961ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:15:18,917] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:15:18,965] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:15:18,970] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:15:29,159] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:15:29,159] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 113 due to node 2 being disconnected (elapsed time since creation: 30005ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:15:29,159] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:15:29,159] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 15:15:38,315] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 15:15:38,316] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:15:38,317] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:15:47,322] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:15:47,322] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:15:47,322] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:15:48,914] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:15:48,915] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 60002ms, elapsed time since send: 60002ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:15:48,918] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:15:48,918] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 15:15:48,919] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 15:15:48,920] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:15:48,920] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:15:48,920] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:15:48,921] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Call(callName=createTopics, deadlineMs=1665407748912, tries=1, nextAllowedTryMs=1665407749415) timed out at 1665407748915 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled createTopics request with correlation id 4 due to node 2 being disconnected
[2022-10-10 15:15:48,922] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:15:48,925] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:15:48,925] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:15:48,925] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:15:48,925] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:15:48,925] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:15:48,925] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:15:48,925] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:15:48,925] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:15:48,925] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:15:48,925] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:15:48,925] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:15:48,925] INFO Kafka startTimeMs: 1665407748925 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:16:08,315] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:16:08,316] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 115 due to node 2 being disconnected (elapsed time since creation: 30005ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:16:08,316] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:16:08,317] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 15:16:17,255] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 15:16:17,256] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:16:17,256] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:16:18,938] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:16:18,940] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 111 due to node 2 being disconnected (elapsed time since creation: 59972ms, elapsed time since send: 59969ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:16:18,941] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:16:18,989] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:16:18,996] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:16:26,261] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:16:26,265] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:16:26,265] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:16:47,256] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:16:47,258] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 117 due to node 2 being disconnected (elapsed time since creation: 30004ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:16:47,258] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:16:47,258] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 15:16:48,936] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:16:48,937] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 60001ms, elapsed time since send: 60001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:16:48,937] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:16:48,937] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 15:16:48,937] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 15:16:48,938] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:16:48,938] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:16:48,938] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:16:48,938] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Call(callName=createTopics, deadlineMs=1665407808935, tries=1, nextAllowedTryMs=1665407809437) timed out at 1665407808937 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled createTopics request with correlation id 4 due to node 2 being disconnected
[2022-10-10 15:16:48,939] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:16:48,944] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:16:48,944] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:16:48,944] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:16:48,944] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:16:48,944] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:16:48,944] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:16:48,944] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:16:48,945] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:16:48,945] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:16:48,945] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:16:48,945] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:16:48,945] INFO Kafka startTimeMs: 1665407808945 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:16:56,353] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 15:16:56,353] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:16:56,354] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:17:05,358] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:17:05,360] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:17:05,360] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:17:18,954] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:17:18,956] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 115 due to node 2 being disconnected (elapsed time since creation: 59965ms, elapsed time since send: 59959ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:17:18,956] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:17:19,004] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:17:19,007] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:17:26,354] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:17:26,354] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 119 due to node 2 being disconnected (elapsed time since creation: 30003ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:17:26,354] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:17:26,355] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 15:17:35,369] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 15:17:35,370] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:17:35,370] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:17:44,373] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:17:44,374] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:17:44,375] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:17:48,954] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:17:48,954] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 60001ms, elapsed time since send: 60001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:17:48,955] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:17:48,955] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 15:17:48,955] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 15:17:48,956] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:17:48,956] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:17:48,956] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:17:48,958] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Call(callName=createTopics, deadlineMs=1665407868953, tries=1, nextAllowedTryMs=1665407869454) timed out at 1665407868954 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled createTopics request with correlation id 4 due to node 2 being disconnected
[2022-10-10 15:17:48,959] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:17:48,960] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:17:48,960] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:17:48,960] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:17:48,960] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:17:48,960] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:17:48,960] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:17:48,961] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:17:48,961] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:17:48,961] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:17:48,961] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:17:48,961] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:17:48,961] INFO Kafka startTimeMs: 1665407868961 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:18:05,370] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:18:05,371] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 121 due to node 2 being disconnected (elapsed time since creation: 30003ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:18:05,371] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:18:05,371] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 15:18:14,278] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 15:18:14,279] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:18:14,279] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:18:18,971] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:18:18,972] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 119 due to node 2 being disconnected (elapsed time since creation: 59967ms, elapsed time since send: 59964ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:18:18,972] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:18:19,024] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:18:19,028] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:18:23,280] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:18:23,280] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:18:23,280] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:18:44,279] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:18:44,279] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 123 due to node 2 being disconnected (elapsed time since creation: 30005ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:18:44,279] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:18:44,280] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 15:18:48,971] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:18:48,971] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 60000ms, elapsed time since send: 60000ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:18:48,972] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:18:48,972] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 15:18:48,972] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 15:18:48,973] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:18:48,974] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:18:48,974] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:18:48,974] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Call(callName=createTopics, deadlineMs=1665407928970, tries=1, nextAllowedTryMs=1665407929471) timed out at 1665407928971 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled createTopics request with correlation id 4 due to node 2 being disconnected
[2022-10-10 15:18:48,978] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:18:48,981] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:18:48,981] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:18:48,981] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:18:48,981] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:18:48,981] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:18:48,981] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:18:48,981] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:18:48,981] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:18:48,981] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:18:48,981] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:18:48,981] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:18:48,982] INFO Kafka startTimeMs: 1665407928981 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:18:53,247] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 15:18:53,247] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:18:53,248] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:19:02,252] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:19:02,252] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:19:02,253] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:19:18,994] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:19:18,996] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 123 due to node 2 being disconnected (elapsed time since creation: 59970ms, elapsed time since send: 59967ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:19:18,996] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:19:19,045] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:19:19,049] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:19:23,248] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:19:23,249] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 125 due to node 2 being disconnected (elapsed time since creation: 30003ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:19:23,249] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:19:23,249] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 15:19:32,279] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 15:19:32,280] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:19:32,281] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:19:41,284] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:19:41,285] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:19:41,286] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:19:48,993] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:19:48,994] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 60001ms, elapsed time since send: 60001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:19:48,994] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:19:48,994] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 15:19:48,994] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 15:19:48,995] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:19:48,995] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:19:48,995] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:19:48,996] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Call(callName=createTopics, deadlineMs=1665407988992, tries=1, nextAllowedTryMs=1665407989494) timed out at 1665407988994 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled createTopics request with correlation id 4 due to node 2 being disconnected
[2022-10-10 15:19:48,997] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:19:48,999] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:19:48,999] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:19:48,999] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:19:48,999] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:19:49,000] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:19:49,000] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:19:49,000] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:19:49,000] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:19:49,000] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:19:49,000] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:19:49,000] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:19:49,000] INFO Kafka startTimeMs: 1665407989000 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:20:19,010] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:20:19,010] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 127 due to node 2 being disconnected (elapsed time since creation: 59964ms, elapsed time since send: 59961ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:20:19,010] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:20:19,061] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:20:19,083] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:20:32,281] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:20:32,281] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 127 due to node 2 being disconnected (elapsed time since creation: 60005ms, elapsed time since send: 60001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:20:32,281] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:20:32,281] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 15:20:41,306] INFO [Controller 2] The request from broker 2 to unfence has been granted because it has caught up with the last committed metadata offset 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 15:20:41,306] INFO [Controller 2] handleBrokerUnfenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:20:41,306] INFO [Controller 2] Unfenced broker: UnfenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:20:49,008] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:20:49,008] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 60001ms, elapsed time since send: 60001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:20:49,009] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:20:49,009] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 15:20:49,009] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 15:20:49,011] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:20:49,011] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:20:49,011] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:20:49,011] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Call(callName=createTopics, deadlineMs=1665408049007, tries=1, nextAllowedTryMs=1665408049508) timed out at 1665408049008 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled createTopics request with correlation id 4 due to node 2 being disconnected
[2022-10-10 15:20:49,013] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:20:49,015] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:20:49,015] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:20:49,016] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:20:49,016] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:20:49,016] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:20:49,016] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:20:49,016] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:20:49,016] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:20:49,016] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:20:49,016] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:20:49,016] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:20:49,016] INFO Kafka startTimeMs: 1665408049016 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:20:50,310] INFO [Controller 2] Fencing broker 2 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:20:50,310] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:20:50,311] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:21:11,307] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:21:11,307] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 129 due to node 2 being disconnected (elapsed time since creation: 30004ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:21:11,308] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:21:11,308] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 15:21:12,297] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler)
[2022-10-10 15:21:12,300] INFO SbcEventQueueKafkaEventQueue#close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2022-10-10 15:21:12,316] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:21:12,316] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:21:12,316] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:21:12,317] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:21:12,317] INFO Closing reporter io.confluent.telemetry.reporter.TelemetryReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:21:12,317] INFO Stopping TelemetryReporter collectorTask (io.confluent.telemetry.reporter.TelemetryReporter)
[2022-10-10 15:21:12,317] INFO Closing the event logger (io.confluent.telemetry.reporter.TelemetryReporter)
[2022-10-10 15:21:12,320] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:21:12,320] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:21:12,321] INFO SbcEventQueueclosed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2022-10-10 15:21:12,321] INFO [BrokerServer id=2] Transition from STARTED to SHUTTING_DOWN (kafka.server.BrokerServer)
[2022-10-10 15:21:12,321] INFO [BrokerServer id=2] shutting down (kafka.server.BrokerServer)
[2022-10-10 15:21:12,322] INFO Closing License Store (io.confluent.license.LicenseStore)
[2022-10-10 15:21:12,322] INFO Stopping KafkaBasedLog for topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog)
[2022-10-10 15:21:12,322] INFO [Producer clientId=_confluent-license-producer-2] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
[2022-10-10 15:21:12,323] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:21:12,323] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:21:12,323] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:21:12,323] INFO App info kafka.producer for _confluent-license-producer-2 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:21:12,323] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:21:12,323] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:21:12,323] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:21:12,323] INFO App info kafka.consumer for _confluent-license-consumer-2 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:21:12,323] INFO Stopped KafkaBasedLog for topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog)
[2022-10-10 15:21:12,323] INFO Closed License Store (io.confluent.license.LicenseStore)
[2022-10-10 15:21:12,324] INFO [BrokerMetadataListener id=2] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2022-10-10 15:21:12,324] INFO [BrokerLifecycleManager id=2] Beginning controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2022-10-10 15:21:12,326] INFO [Controller 2] Fenced broker 2 has requested and been granted an immediate shutdown. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 15:21:12,326] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:21:12,326] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
[2022-10-10 15:21:19,033] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:21:19,033] INFO [BrokerToControllerChannelManager broker=2 name=forwarding] Cancelled in-flight ENVELOPE request with correlation id 131 due to node 2 being disconnected (elapsed time since creation: 59971ms, elapsed time since send: 59949ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:21:19,033] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:21:19,083] INFO [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:21:19,084] INFO [Controller 2] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, mirrorTopicId=AAAAAAAAAAAAAAAAAAAAAA): TOPIC_ALREADY_EXISTS (Topic '_confluent-telemetry-metrics' already exists.) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:21:42,328] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:21:42,329] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 131 due to node 2 being disconnected (elapsed time since creation: 30003ms, elapsed time since send: 30001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:21:42,329] INFO [BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:19093 (id: 2 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-10-10 15:21:42,330] INFO [BrokerLifecycleManager id=2] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2022-10-10 15:21:49,032] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:21:49,033] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight CREATE_TOPICS request with correlation id 4 due to node 2 being disconnected (elapsed time since creation: 60001ms, elapsed time since send: 60001ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-10-10 15:21:49,034] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:21:49,034] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
[2022-10-10 15:21:49,034] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
[2022-10-10 15:21:49,036] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:21:49,036] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:21:49,036] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-10-10 15:21:49,036] ERROR Error checking or creating telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
org.apache.kafka.common.errors.TimeoutException: Call(callName=createTopics, deadlineMs=1665408109031, tries=1, nextAllowedTryMs=1665408109533) timed out at 1665408109033 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled createTopics request with correlation id 4 due to node 2 being disconnected
[2022-10-10 15:21:49,037] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:21:49,039] WARN The configuration 'compression.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:21:49,040] WARN The configuration 'enable.idempotence' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:21:49,040] WARN The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:21:49,040] WARN The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:21:49,040] WARN The configuration 'max.request.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:21:49,040] WARN The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:21:49,040] WARN The configuration 'interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:21:49,040] WARN The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:21:49,040] WARN The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-10-10 15:21:49,040] INFO Kafka version: 7.2.2-ce (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:21:49,040] INFO Kafka commitId: be084aaf34d44455cf73e11f4516f13aa64615d8 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:21:49,040] INFO Kafka startTimeMs: 1665408109040 (org.apache.kafka.common.utils.AppInfoParser)
[2022-10-10 15:21:51,380] INFO [Controller 2] Fenced broker 2 has requested and been granted an immediate shutdown. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-10-10 15:21:51,380] INFO [Controller 2] handleBrokerFenced: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
[2022-10-10 15:21:51,380] INFO [Controller 2] Fenced broker: FenceBrokerRecord(id=2, epoch=2) (org.apache.kafka.controller.ClusterControlManager)
